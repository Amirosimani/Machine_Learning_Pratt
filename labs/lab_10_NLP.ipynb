{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd21279-0580-4a0b-9b03-4238dcc53533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %conda install -c conda-forge ipywidgets\n",
    "# %jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfbe7d8",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0068bfb1",
   "metadata": {},
   "source": [
    "Natural language processing (NLP) is the ability of a computer program to understand human language as it is spoken and written.\n",
    "\n",
    "Language is considered **unstructured** data. Unstructured data is information that is not arranged according to a preset data model or schema, and therefore cannot be stored in a traditional relational database (think excel files).\n",
    "Almost most of the data generated and collected is unstructured.\n",
    "\n",
    "<img src=\"./img/lab_10_nlp_history.png\">\n",
    "\n",
    "\n",
    "A main challenge in NLP is how to represent text as data that is consumable by the computer understands.\n",
    "\n",
    "-----\n",
    "\n",
    "\n",
    "There are two main phases to natural language processing: **data representation** and **algorithm development**.\n",
    "\n",
    "## Data representation\n",
    "In order to perform machine learning on text documents, we first need to turn the text content into numerical feature vectors. There are mulitple approaches this can be done, including:\n",
    "\n",
    "* **Tokenization**: This is when text is broken down into smaller units to work with.\n",
    "\n",
    "<img src=\"./img/lab_10_tokenization.png\">\n",
    "\n",
    "After you decided on your tokenization strategy, you have to preprocess the tokens. Here are a few common preprocessing approaches.\n",
    "\n",
    "* **Lowercasing**: lowercase all the text data\n",
    "* **Stop Word Removal**:This is when common words are removed from text so unique words that offer the most information about the text remain.\n",
    "* **Lemmatization & Stemming**: This is when words are reduced to their root forms to process.\n",
    "* **Part-of-Speech (POS) Tagging**: This is when words are marked based on the part-of speech they are -- such as nouns, verbs and adjectives.\n",
    "\n",
    "\n",
    "### Still, how can we turn these tokens to numbers that retain their meaning?\n",
    "\n",
    "#### **one-hot encoding**:\n",
    " on-hot encode each word in the sentence. The steps are as follow:\n",
    "    * First, create a list with the size of our vocabulary.\n",
    "    * Assign 1 one for words that exists in the sentence.\n",
    "\n",
    "|        | chase | dog | person | word | ... |\n",
    "|--------|-------|-----|--------|------|-----|\n",
    "| dog    | 0     | 1   | 0      | 0    | 0   |\n",
    "| chase  | 1     | 0   | 0      | 0    | 0   |\n",
    "| person | 0     | 0   | 1      | 0    | 0   |\n",
    "\n",
    "We converted \"Dog chase person\" to a matrix!\n",
    "\n",
    "What are the issues with this approach?\n",
    "- This representation does not convey any relationships between words\n",
    "- The generated matrix is high-dimensional and sparse\n",
    "\n",
    "\n",
    "#### **Bag-of-Words**:\n",
    "BoW is a simple document embedding technique based on word frequency.\n",
    "* Create a vector whose length is equal to the size of the vocabulary\n",
    "* Place a value to represent the frequency in which the word appears in the given document\n",
    "\n",
    "Let's look at a new example `My dog is chasing his dog`. You can create a BoW representation like this:\n",
    "\n",
    "| chase | cat | dog | his | person | my | word | ... |\n",
    "|-------|-----|-----|-----|--------|----|------|-----|\n",
    "| 1     | 0   | 2   | 1   | 0      | 1  | 0    | 0   |\n",
    "\n",
    "The output vector is `[1, 0, 2, 1, 0, 1, 0, 0, ...]`\n",
    "\n",
    "* This approach captures `shallow` semantics i.e. If two sentences have similar vocabulary, the two vectors that represent them are close in the vector space and they might have similar meanings.\n",
    "* The generated matrix is less sparse compared to one-hot encoding.\n",
    "\n",
    "Still it is sparse, doesn't fully capture the semantics (`My dog is chasing his dog` vs `His dog is chasing my dog`)\n",
    "\n",
    "#### **Word Embeddings**: \n",
    "\n",
    "A technique to represent words in low-dimensional dense vectors while capturing the relationship between the words in the vector space.\n",
    "\n",
    "There are many approaches to generate word embeddings like `word2vec`, `GloVe`, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae75772",
   "metadata": {},
   "source": [
    "\n",
    "## Algorithms\n",
    "Natural language processing applies algorithms to understand the meaning and structure of sentences. These algorithms include:\n",
    "\n",
    "* **Word sense disambiguation**. This derives the meaning of a word based on context.\n",
    "* **Named entity recognition**. This determines words that can be categorized into groups.\n",
    "* **Natural language generation**. This is used to determine semantics behind words and generate new text.\n",
    "* **Text classification**. This involves assigning tags to texts to put them in categories. This can be useful for sentiment analysis, which helps the natural language processing algorithm determine the sentiment, or emotion behind a text. \n",
    "* **Text extraction**. This involves automatically summarizing text and finding important pieces of data.\n",
    "* **Machine translation**. This is the process by which a computer translates text from one language, such as English, to another language, such as French, without human intervention.\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609926be-e827-40e1-9241-2aac57472f89",
   "metadata": {},
   "source": [
    "## What is Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259a701b-6622-4b68-822b-7f2c42f43d68",
   "metadata": {},
   "source": [
    "# What is BERT?\n",
    "\n",
    "<img src=\"./img/lab_12_bert.jpg\">\n",
    "\n",
    "**BERT** stands for Bidirectional Encoder Representations from Transformers. Jacob Devlin and his colleagues developed BERT at Google in 2018. Devlin and his colleagues trained the BERT on English Wikipedia (2,500M words) and BooksCorpus (800M words) and achieved the best accuracies for some of the NLP tasks in 2018. \n",
    "\n",
    "\n",
    "There are two pre-trained general BERT variations: The base model is a 12-layer, 768-hidden, 12-heads, 110M parameter neural network architecture, whereas the large model is a 24-layer, 1024-hidden, 16-heads, 340M parameter neural network architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ac28b4-46e7-4607-84b6-957730cceb13",
   "metadata": {},
   "source": [
    "# What is Hugging Face ðŸ¤—\n",
    "\n",
    "Hugging Face is an open-source provider of natural language processing (NLP) technologies. It has a large open-source community, in particular around the Transformers library.\n",
    "\n",
    "ðŸ¤—/Transformers is a python-based library that exposes an API to use many well-known transformer architectures, such as BERT, RoBERTa, GPT-2 or DistilBERT, that obtain state-of-the-art results on a variety of NLP tasks like text classification, information extraction, question answering, and text generation. Those architectures come pre-trained with several sets of weights. Getting started with Transformers only requires to install the pip package:\n",
    "\n",
    "`pip install transformers`\n",
    "\n",
    "\n",
    "more here: https://blog.tensorflow.org/2019/11/hugging-face-state-of-art-natural.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f607a7f2-c212-4373-afe2-40ee3d297c80",
   "metadata": {},
   "source": [
    "The advantage of using `Transformers` lies in the straight-forward model-agnostic API. Loading a pre-trained model, along with its tokenizer can be done in a few lines of code. Here is an example of loading the BERT TensorFlow models as well as their tokenizers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02920a72-2843-4270-ada2-8c744dc6c892",
   "metadata": {},
   "source": [
    "# Fine-tuning a pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031533b3-afce-4f95-a878-1e73ff845701",
   "metadata": {},
   "source": [
    "We are gping to use the IMDB dataset: the task is to classify whether movie reviews are positive or negative. For more infromation you can check Datasets [documentation](https://huggingface.co/docs/datasets/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7330005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (2.14.6)\n",
      "Requirement already satisfied: transformers in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (4.34.1)\n",
      "Requirement already satisfied: tensorflow in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (2.14.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from datasets) (1.25.2)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from datasets) (13.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from datasets) (2.1.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from datasets) (0.17.3)\n",
      "Requirement already satisfied: packaging in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: filelock in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tensorflow-macos==2.14.0 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from tensorflow) (2.14.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from tensorflow-macos==2.14.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from tensorflow-macos==2.14.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from tensorflow-macos==2.14.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from tensorflow-macos==2.14.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from tensorflow-macos==2.14.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from tensorflow-macos==2.14.0->tensorflow) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from tensorflow-macos==2.14.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes==0.2.0 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from tensorflow-macos==2.14.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from tensorflow-macos==2.14.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from tensorflow-macos==2.14.0->tensorflow) (4.24.4)\n",
      "Requirement already satisfied: setuptools in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from tensorflow-macos==2.14.0->tensorflow) (58.0.4)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from tensorflow-macos==2.14.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from tensorflow-macos==2.14.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from tensorflow-macos==2.14.0->tensorflow) (4.7.1)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from tensorflow-macos==2.14.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from tensorflow-macos==2.14.0->tensorflow) (0.34.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from tensorflow-macos==2.14.0->tensorflow) (1.59.2)\n",
      "Requirement already satisfied: tensorboard<2.15,>=2.14 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from tensorflow-macos==2.14.0->tensorflow) (2.14.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from tensorflow-macos==2.14.0->tensorflow) (2.14.0)\n",
      "Requirement already satisfied: keras<2.15,>=2.14.0 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from tensorflow-macos==2.14.0->tensorflow) (2.14.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from aiohttp->datasets) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow-macos==2.14.0->tensorflow) (0.41.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (2.23.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (3.5)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (6.8.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (3.16.2)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow) (3.2.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49067b9a-1870-4f4a-a92e-fc03957e6982",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/amirimani/Documents/projects/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# download and cache the dataset:\n",
    "raw_datasets = load_dataset(\"imdb\")\n",
    "\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f013160b-3d75-46e7-b448-efd55cfb866a",
   "metadata": {},
   "source": [
    "To preprocess our data, we will need a tokenizer. If you plan on using a pretrained model, itâ€™s important to use the associated pretrained tokenizer: it will split the text you give it in tokens the same way for the pretraining corpus, and it will use the same correspondence token to index (that we usually call a vocab) as during pretraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "131ef06c-c91b-43fe-8701-a592c7e874e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "#  automatically download the vocab used during pretraining or fine-tuning a given model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abfe5951-4123-4988-9aa5-8879e2dba8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 146, 1567, 3395, 3776, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "encoded_input = tokenizer(\"I love machine learning!\")\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e62512-673a-4297-a9ab-50333fbffe00",
   "metadata": {},
   "source": [
    "This returns a dictionary string to list of ints. The input_ids are the indices corresponding to each token in our sentence. We will see below what the attention_mask is used for and in the next section the goal of token_type_ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bfe6509-0f6f-4480-bc42-5a56aaeed1df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] I love machine learning! [SEP]'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b86004-9955-4bc3-a48f-ca50c4bb8bc5",
   "metadata": {},
   "source": [
    "As you can see, the tokenizer automatically added some special tokens that the model expects. Now let's tokenize our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0157ba95-cc61-4141-8c76-1f8c82c8ad29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25000/25000 [00:05<00:00, 4738.74 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# This will make all the samples have the maximum length the model can accept (here 512),\n",
    "# either by padding or truncating them. Note that we are applying the preprocessing step to\n",
    "# all splots of the raw dataset\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccf61eb8-4759-467e-a2d2-61c74a5e6c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n",
    "# full_train_dataset = tokenized_datasets[\"train\"]\n",
    "# full_eval_dataset = tokenized_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c3991a5-c4bb-49b8-9d79-a1f12cf4ded5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 436M/436M [00:14<00:00, 29.8MB/s] \n",
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", \n",
    "                                                             num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d33dc069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  108310272 \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 108311810 (413.18 MB)\n",
      "Trainable params: 108311810 (413.18 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5de5f2-ac87-4751-9c15-fb59e203c49c",
   "metadata": {},
   "source": [
    "Since we are going to train our model natively in TensorFlow, we need to convert our datasets to standard `td.data.Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4464ce22-f03a-432d-9a2e-4d490a795bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_dataset = small_train_dataset.remove_columns([\"text\"]).with_format(\"tensorflow\")\n",
    "tf_eval_dataset = small_eval_dataset.remove_columns([\"text\"]).with_format(\"tensorflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61b507f1-2338-4a6d-b055-1f1fd14df8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = {x: tf_train_dataset[x] for x in tokenizer.model_input_names}\n",
    "train_tf_dataset = tf.data.Dataset.from_tensor_slices((train_features, tf_train_dataset[\"label\"]))\n",
    "train_tf_dataset = train_tf_dataset.shuffle(len(tf_train_dataset)).batch(8)\n",
    "\n",
    "eval_features = {x: tf_eval_dataset[x] for x in tokenizer.model_input_names}\n",
    "eval_tf_dataset = tf.data.Dataset.from_tensor_slices((eval_features, tf_eval_dataset[\"label\"]))\n",
    "eval_tf_dataset = eval_tf_dataset.batch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5bb26e70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1000, 512), dtype=int64, numpy=\n",
       " array([[ 101, 1247, 1110, ...,    0,    0,    0],\n",
       "        [ 101, 1188, 2523, ...,    0,    0,    0],\n",
       "        [ 101, 1667,  153, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 101, 8762, 1126, ...,    0,    0,    0],\n",
       "        [ 101, 1109, 1646, ...,    0,    0,    0],\n",
       "        [ 101,  146, 1138, ...,  117, 1176,  102]])>,\n",
       " 'token_type_ids': <tf.Tensor: shape=(1000, 512), dtype=int64, numpy=\n",
       " array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]])>,\n",
       " 'attention_mask': <tf.Tensor: shape=(1000, 512), dtype=int64, numpy=\n",
       " array([[1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 1, 1, 1]])>}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcbd4955-8ca3-4718-81dc-7803ce14c177",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      " 21/125 [====>.........................] - ETA: 14:44 - loss: 0.7522 - sparse_categorical_accuracy: 0.4940"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=tf.metrics.SparseCategoricalAccuracy(),\n",
    ")\n",
    "\n",
    "model.fit(train_tf_dataset, validation_data=eval_tf_dataset, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feba0e18-4618-4315-962a-45909efed3a0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8f64b7-d002-4d28-bdee-cea2b4786427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the fine-tuned model for future use\n",
    "# model.save_pretrained(\"./my_imdb_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571764a6-e4b6-40af-9baf-2c5f153a690c",
   "metadata": {},
   "source": [
    "There are many more examples for different tasks sucj as text classification, question answering, etc [here](https://github.com/huggingface/transformers/tree/master/examples/tensorflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2454f68e-93f3-4398-b99b-de265fdeec7d",
   "metadata": {},
   "source": [
    "## optional - HugginFace approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62aa843e-da8c-4fee-93b5-3398fc340b0c",
   "metadata": {},
   "source": [
    "compute_metrics function takes predictions and labels and computes and returns a dictionary with string items (the metric names) and float values (the metric values).\n",
    "\n",
    "The ðŸ¤— Datasets library provides an easy way to get the common metrics used in NLP with the load_metric function. here we simply use accuracy. Then we define the compute_metrics function that just convert logits to predictions (remember that all ðŸ¤— Transformers models return the logits) and feed them to compute method of this metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb87e8b2-0456-46d3-8624-5479010a61d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16f9eb1-7574-46ad-82f1-34262b74d9b2",
   "metadata": {},
   "source": [
    "To define our Trainer, we will need to instantiate a TrainingArguments. This class contains all the hyperparameters we can tune for the Trainer or the flags to activate the different training options it supports. Letâ€™s begin by using all the default arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c92732-905e-41e4-85f0-bb6c04f48cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\"test_trainer\", evaluation_strategy=\"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b32f2fc-1ba6-403c-bb00-d57bc8c426e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "# instantiate a Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args, \n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166109a1-9900-4076-a189-a5fa9d066592",
   "metadata": {},
   "source": [
    "To fine-tune our model, we just need to call\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9cf52c-4563-43e4-bd2c-b563804992fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
