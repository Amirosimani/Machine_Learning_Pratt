{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Lifecycle\n",
    "\n",
    "Machine learning projects are highly iterative; as you progress through the ML lifecycle, you’ll find yourself iterating on a section until reaching a satisfactory level of performance, then proceeding forward to the next task (which may be circling back to an even earlier step)\n",
    "\n",
    "[<img src=\"img/lab_05_cycle.png\"/>](https://www.jeremyjordan.me/ml-projects-guide/)\n",
    "\n",
    "\n",
    "Often times, we don't immediately know what the optimal model architecture should be for a given model, and thus we'd like to be able to explore a range of possibilities. **Start simple and gradually ramp up complexity**. This typically involves using a simple model, but can also include starting with a simpler version of your task, and data preparation.\n",
    "\n",
    "Once you have a general idea of successful model architectures and approaches for your problem, you should now spend much more focused effort on squeezing out performance gains including from the model. That's why we use **pipelines**.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing & Feature Engineering\n",
    "\n",
    "Preprocessing and feature engineering efforts mainly have two goals:\n",
    "\n",
    "1. Preparing the proper input dataset, compatible with the machine learning algorithm requirements.\n",
    "2. Improving the performance of machine learning models.\n",
    "\n",
    "\n",
    "There are different techniques and steps for preprocessing and feature engineering based on the dataset and/or algorithms.\n",
    "\n",
    "Here is a list of major feature engineering techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "\n",
    "In most cases, the numerical features of the dataset do not have a certain range and they differ from each other.\n",
    "\n",
    "For example, **age** and **income** don't have the same range in real life, but from the machine learning point of view, we need a way to compare these two columns.\n",
    "\n",
    "Scaling solves this problem. The continuous features become identical in terms of the range, after a scaling process. This process is not mandatory for many algorithms, but in general, ML models benefit from scaling of the data.\n",
    "\n",
    "Remeber, if your algorithm is based on **distance** calculations such as k-NN or k-Means need to have scaled continuous features as model input.\n",
    "\n",
    "* **Normalization**:  is the process of scaling individual samples to have unit norm.\n",
    "\n",
    "X_new = (X - X_min)/(X_max - X_min)\n",
    "\n",
    "* **Standardization**: Standardization of datasets is a common requirement for many machine learning estimators implemented in scikit-learn; they might behave badly if the individual features do not more or less look like standard normally distributed data: Gaussian with **zero mean and unit variance**.\n",
    "X_new = (X - mean)/Std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "data = [[0, 0], [0, 0], [1, 1], [1, 1]]\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(data)\n",
    "print(f\"Original data: {data}\")\n",
    "\n",
    "print(f\"Mean: {scaler.mean_}\")\n",
    "\n",
    "scaler.transform(data)\n",
    "\n",
    "\n",
    "# print(scaler.transform([[2, 2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Imputation\n",
    "\n",
    "Missing values are one of the most common problems you can encounter when you try to prepare your data for machine learning. The reason for the missing values might be human errors, interruptions in the data flow, privacy concerns, and so on. Whatever is the reason, missing values affect the performance of the machine learning models.\n",
    "\n",
    "**Dropping missing values**: The most simple solution to the missing values is to drop the rows or the entire column.\n",
    "\n",
    "**Numerical imputation**: using Mean/Median values. This works by calculating the mean/median of the non-missing **values in** a column and then replacing the missing values within each column separately and independently from the others. It can only be used with numeric data.\n",
    "\n",
    "\n",
    "**Categorical imputation**: Replacing the missing values with the maximum occurred value in a column is a good option for handling categorical columns.\n",
    "\n",
    "The above methods don’t factor the correlations between features. It only works on the column level.\n",
    "\n",
    "**ML-based imputation** for example, **K-NN** uses ‘feature similarity’ to predict the values of any new data points. This means that the new point is assigned a value based on how closely it resembles the points in the training set. This can be very useful in making predictions about the missing values by finding the k’s closest neighbours to the observation with missing data and then imputing them based on the non-missing values in the neighbourhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp.fit([[1, 2], [np.nan, 3], [7, 6]])\n",
    "\n",
    "X = [[np.nan, 2], [6, np.nan], [7, 6]]\n",
    "print(imp.transform(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Handling outliers\n",
    "\n",
    "Many applications require being able to decide whether a new observation belongs to the same distribution as existing observations (it is an inlier), or should be considered as different (it is an outlier). Often, this ability is used to clean real data sets.. Outliers in data can have many causes such as 1-data entry error (human error), 2-Measurement errors (instrument errors), 3- Experimental errors, 4- Data processing errors, 4- Sampling errors, 5. Natural (not an error, novelties in data) and so on.\n",
    "\n",
    "Usually the best way to detect the outliers is to demonstrate the data visually (Histograms, Scatter plot, box plots, ...). However, if visualization is not possible, there are statistical methods to detect outliers such as:\n",
    "* Z-Score or Extreme Value Analysis (parametric): The z-score or standard score of an observation is a metric that indicates how many standard deviations a data point is from the sample’s mean, assuming a gaussian distribution.\n",
    "\n",
    "\n",
    "Often outliers are discarded because of their effect on the total distribution and statistical analysis of the dataset. This is certainly a good approach if the outliers are due to an error of some kind (measurement error, data corruption, etc.), however often the source of the outliers is unclear. There are many situations where occasional ‘extreme’ events cause an outlier that is outside the usual distribution of the dataset but is a valid measurement and not due to an error. In these situations, the choice of how to deal with the outliers is not necessarily clear and the choice has a significant impact on the results of any statistical analysis done on the dataset. The decision about how to deal with outliers depends on the goals and context of the research and should be detailed in any explanation about the methodology.\n",
    "\n",
    "\n",
    "There are some techniques used to deal with outliers.\n",
    "* Deleting observations\n",
    "* Transforming values: Use **robust** transformers\n",
    "* Separately treating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "#Load data\n",
    "X, y = load_boston(return_X_y=True)\n",
    "\n",
    "#Create data frame\n",
    "boston = load_boston()\n",
    "columns = boston.feature_names\n",
    "df = pd.DataFrame(X, columns = columns)\n",
    "\n",
    "ax = sns.boxplot(data=df[['CRIM', 'ZN']], orient=\"h\", palette=\"Set2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Binning\n",
    "\n",
    "The main motivation of binning is to make the model more robust and prevent overfitting, however, it has a cost to the performance. Every time you bin something, you sacrifice information and make your data more regularized.\n",
    "\n",
    "\n",
    "Binning can be applied on both categorical and numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CRIM'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KBinsDiscretizer can then be used to convert the floating values into fixed number of discrete categories with an ranked ordinal relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer  \n",
    "\n",
    "est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='quantile')\n",
    "df['CRIM_binned'] = est.fit_transform(df[['CRIM']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CRIM_binned'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way is to use `Pandas qcut`. The function defines the bins using percentiles based on the distribution of the data, not the actual numeric edges of the bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CRIM_binned_pd'] = pd.qcut(df['CRIM'], 3, labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CRIM_binned_pd'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## Encoding categorical features\n",
    "\n",
    "Often features are not given as continuous values but categorical.\n",
    "\n",
    "**OrdinalEncoder** is used to convert categorical features to such integer codes. This estimator transforms each categorical feature to one new feature of integers (0 to n_categories - 1)\n",
    "\n",
    "\n",
    "**One-hot encoding** is one of the most common encoding methods in machine learning. This method spreads the values in a column to multiple flag columns and assigns 0 or 1 to them. These binary values express the relationship between grouped and encoded column.\n",
    "\n",
    "This method changes your categorical data, which is challenging to understand for algorithms, to a numerical format and enables you to group your categorical data without losing any information. (For details please see the last part of Categorical Column Grouping)\n",
    "\n",
    "<img src=\"img/lab_05_onehot.png\"/>\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from numpy import asarray\n",
    "\n",
    "# define data\n",
    "data = asarray([['good'], ['better'], ['best']])\n",
    "print(data)\n",
    "# define ordinal encoding\n",
    "encoder = OrdinalEncoder()\n",
    "# transform data\n",
    "result = encoder.fit_transform(data)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# define data\n",
    "data = asarray([['red'], ['green'], ['blue']])\n",
    "print(data)\n",
    "# define one hot encoding\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "# transform data\n",
    "onehot = encoder.fit_transform(data)\n",
    "print(onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is that this representation includes redundancy. For example, if we know that [1, 0, 0] represents “blue” and [0, 1, 0] represents “green” we don’t need another binary variable to represent “red“, instead we could use 0 values for both “blue” and “green” alone, e.g. [0, 0].\n",
    "\n",
    "This is called a **dummy variable encoding**, and always represents C categories with C-1 binary variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of a dummy variable encoding\n",
    "from numpy import asarray\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# define data\n",
    "data = asarray([['red'], ['green'], ['blue']])\n",
    "print(data)\n",
    "# define one hot encoding\n",
    "encoder = OneHotEncoder(drop='first', sparse=False)\n",
    "# transform data\n",
    "onehot = encoder.fit_transform(data)\n",
    "print(onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding/getting dummies Pandas way..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.DataFrame(data)\n",
    "print(df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(df_data,  drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <font color='red'>Interatction/new features </font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Feature Selection\n",
    "\n",
    "Feature selection is the process of reducing the number of input variables when developing a predictive model.\n",
    "\n",
    "It is desirable to reduce the number of input variables to both reduce the computational cost of modeling and, in some cases, to improve the performance of the model.\n",
    "\n",
    "https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "A model **hyperparameter** is a configuration that is external to the model and whose value cannot be estimated from data.\n",
    "\n",
    "* They are often used in processes to help estimate model parameters.\n",
    "* They are often specified by the practitioner.\n",
    "* They can often be set using heuristics.\n",
    "* They are often tuned for a given predictive modeling problem.\n",
    "\n",
    "Hyperparameters address model design questions such as:\n",
    "* K in K-NN\n",
    "* What should be the maximum depth allowed for my decision tree?\n",
    "* How many trees should I include in my random forest?\n",
    "* How many layers should I have in my neural network?\n",
    "\n",
    "\n",
    "We cannot know the best value for a model hyperparameter on a given problem. We may use rules of thumb, copy values used on other problems, or search for the best value by trial and error.\n",
    "\n",
    "[<img src=\"img/lab_05_hpt.png\"/>](https://www.analyticsvidhya.com/blog/2021/04/evaluating-machine-learning-models-hyperparameter-tuning/)\n",
    "\n",
    "-----\n",
    "Searching for the best hyper-parameter can be tedious, hence search algorithms like **grid search** or **random search**. USing these, you are tuning the hyperparameters of the model in order to discover the parameters of the model that result in the most skillful predictions.\n",
    "\n",
    "Use **coarse-to-fine random searches** for hyperparameters. Start with a wide hyperparameter space initially and iteratively hone in on the highest-performing region of the hyperparameter space.\n",
    "\n",
    "Please note that optimal hyperparameters often differ for different datasets and models.\n",
    "\n",
    "A search consists of:\n",
    "\n",
    "* an estimator (regressor or classifier such as sklearn.svm.SVC());\n",
    "* a parameter space;\n",
    "* a method for searching or sampling candidates;\n",
    "* a cross-validation scheme; and\n",
    "* a score function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "\n",
    "data = load_breast_cancer()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.data, data.target, stratify=data.target, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter (CV score=0.982):\n",
      "{'logreg__C': 0.046415888336127774, 'logreg__class_weight': 'balanced', 'logreg__penalty': 'l2'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/imaniai/opt/anaconda3/envs/pratt/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "40 fits failed out of a total of 80.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "40 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/imaniai/opt/anaconda3/envs/pratt/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/imaniai/opt/anaconda3/envs/pratt/lib/python3.8/site-packages/sklearn/pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/Users/imaniai/opt/anaconda3/envs/pratt/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/imaniai/opt/anaconda3/envs/pratt/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/imaniai/opt/anaconda3/envs/pratt/lib/python3.8/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.92501791        nan 0.39870978        nan 0.98234996        nan\n",
      " 0.96922391        nan 0.9497495         nan 0.95465064        nan\n",
      " 0.94485661        nan 0.94496151        nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define a pipeline to search for the best combination of PCA truncation\n",
    "# and classifier regularization.\n",
    "\n",
    "# set the tolerance to a large value to make the example faster\n",
    "logistic = LogisticRegression(max_iter=100, tol=0.1)\n",
    "pipe = Pipeline(steps=[('scaler', StandardScaler()),\n",
    "                       ('logreg', logistic)])\n",
    "\n",
    "# Parameters of pipelines can be set using ‘__’ separated parameter names:\n",
    "param_grid = {\n",
    "    'logreg__C': np.logspace(-4, 4, 4),\n",
    "    'logreg__penalty':['l2', 'l1'],\n",
    "    'logreg__class_weight':['balanced', 'none']\n",
    "    \n",
    "}\n",
    "search = GridSearchCV(pipe, param_grid,\n",
    "                      scoring='f1_macro',\n",
    "                      cv=5,\n",
    "                      n_jobs=-1)\n",
    "search.fit(X_train, y_train)\n",
    "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.92      0.93        53\n",
      "           1       0.96      0.97      0.96        90\n",
      "\n",
      "    accuracy                           0.95       143\n",
      "   macro avg       0.95      0.95      0.95       143\n",
      "weighted avg       0.95      0.95      0.95       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = search.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('logreg',\n",
       "                 LogisticRegression(C=0.046415888336127774,\n",
       "                                    class_weight='balanced', tol=0.1))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.92      0.91        53\n",
      "           1       0.95      0.93      0.94        90\n",
      "\n",
      "    accuracy                           0.93       143\n",
      "   macro avg       0.92      0.93      0.93       143\n",
      "weighted avg       0.93      0.93      0.93       143\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/imaniai/opt/anaconda3/envs/pratt/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "new_lr = search.best_estimator_['logreg']\n",
    "new_lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = new_lr.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "So far, we built machine learning models and trained it on some data... now what?\n",
    "\n",
    "There are still questions that need to be answered like:\n",
    "\n",
    "* How well is my model doing? Is it a useful model?\n",
    "* Will training my model on more data improve its performance?\n",
    "* Do I need to include more features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "Establish performance baselines on your problem. Baselines are useful for both establishing a lower bound of expected performance (simple model baseline) and establishing a target performance level (human baseline).\n",
    "\n",
    "* Simple baselines include out-of-the-box scikit-learn models (i.e. logistic regression with default parameters) or even simple heuristics (always predict the majority class). Without these baselines, it's impossible to evaluate the value of added model complexity.\n",
    "* If your problem is well-studied, search the literature to approximate a baseline based on published results for very similar tasks/datasets.\n",
    "* If possible, try to estimate human-level performance on the given task. Don't naively assume that humans will perform the task perfectly, a lot of simple tasks are deceptively hard!\n",
    "\n",
    "## Why is my model performing poorly?\n",
    "\n",
    "There a many reasons why your model is not performing as you like it.\n",
    "\n",
    "* Implementation bugs\n",
    "* Hyperparameter choices\n",
    "* Data/model fit\n",
    "* Dataset construction\n",
    "\n",
    "\n",
    "### Train/Test split\n",
    "\n",
    "First step to properly evaluate your model is to not train the model on the entire dataset. I repeat: **do not train the model on the entire dataset.**\n",
    "\n",
    "\n",
    "<img src=\"img/lab_05_split.png\"/>\n",
    "\n",
    "The training set contains a known output and the model learns on this data in order to be generalized to other data later on. We have the test dataset (or subset) in order to test our model’s prediction on this subset.\n",
    "\n",
    "\n",
    "\n",
    "### Metrics\n",
    "\n",
    "There are a variety of metrics you can use to evaluate your regression, clssification, and clustering models. You can read about many of them [here](https://scikit-learn.org/stable/modules/model_evaluation.html)\n",
    "\n",
    "### Regression metrics\n",
    "\n",
    "### Classification metrics\n",
    "* imbalanced data\n",
    "\n",
    "### Clustering metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "data = load_breast_cancer()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.data, data.target, stratify=data.target, random_state=0)\n",
    "\n",
    "lr = LogisticRegression().fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# print(confusion_matrix(y_test, y_pred))\n",
    "print(lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Add noisy features\n",
    "random_state = np.random.RandomState(0)\n",
    "n_samples, n_features = X.shape\n",
    "X = np.c_[X, random_state.randn(n_samples, 200 * n_features)]\n",
    "\n",
    "# Limit to the two first classes, and split into training and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[y < 2], y[y < 2],\n",
    "                                                    test_size=.5,\n",
    "                                                    random_state=random_state)\n",
    "\n",
    "# Create a simple classifier\n",
    "classifier = svm.LinearSVC(random_state=random_state)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_score = classifier.decision_function(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "disp = plot_precision_recall_curve(classifier, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roc curve and auc\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot\n",
    "\n",
    "\n",
    "# generate 2 class dataset\n",
    "X, y = make_classification(n_samples=1000, n_classes=2, random_state=1)\n",
    "# split into train/test sets\n",
    "trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.5, random_state=2)\n",
    "# generate a no skill prediction (majority class)\n",
    "ns_probs = [0 for _ in range(len(testy))]\n",
    "# fit a model\n",
    "model = LogisticRegression(solver='lbfgs')\n",
    "model.fit(trainX, trainy)\n",
    "# predict probabilities\n",
    "lr_probs = model.predict_proba(testX)\n",
    "# keep probabilities for the positive outcome only\n",
    "lr_probs = lr_probs[:, 1]\n",
    "# calculate scores\n",
    "ns_auc = roc_auc_score(testy, ns_probs)\n",
    "lr_auc = roc_auc_score(testy, lr_probs)\n",
    "# summarize scores\n",
    "print('No Skill: ROC AUC=%.3f' % (ns_auc))\n",
    "print('Logistic: ROC AUC=%.3f' % (lr_auc))\n",
    "# calculate roc curves\n",
    "ns_fpr, ns_tpr, _ = roc_curve(testy, ns_probs)\n",
    "lr_fpr, lr_tpr, _ = roc_curve(testy, lr_probs)\n",
    "# plot the roc curve for the model\n",
    "pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "pyplot.plot(lr_fpr, lr_tpr, marker='.', label='Logistic')\n",
    "# axis labels\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "# mammography dataset https://www.openml.org/d/310\n",
    "data = fetch_openml('mammography')\n",
    "X, y = data.data, data.target\n",
    "y = (y.astype(np.int) + 1) // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bincount(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=42)\n",
    "\n",
    "\n",
    "lr = LogisticRegression().fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(class_weight='balanced').fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --user imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "rus = RandomUnderSampler(replacement=False)\n",
    "X_train_subsample, y_train_subsample = rus.fit_sample(X_train, y_train)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_train_subsample.shape)\n",
    "print(np.bincount(y_train_subsample))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression().fit(X_train_subsample, y_train_subsample)\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import make_pipeline as make_imb_pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "oversample_pipe = make_imb_pipeline(RandomOverSampler(),\n",
    "                                    LogisticRegression())\n",
    "\n",
    "oversample_pipe.fit(X_train, y_train)\n",
    "y_pred = oversample_pipe.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_oversample, y_train_oversample = RandomOverSampler().fit_sample(X_train, y_train)\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_train_oversample.shape)\n",
    "print(np.bincount(y_train_oversample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, precision_recall_curve\n",
    "\n",
    "probs_oversample = oversample_pipe.predict_proba(X_test)[:, 1]\n",
    "fpr_over, tpr_over, _ = roc_curve(y_test, probs_oversample)\n",
    "precision_over, recall_over, _ = precision_recall_curve(y_test, probs_oversample)\n",
    "\n",
    "undersample_pipe = make_imb_pipeline(RandomUnderSampler(),\n",
    "                                    LogisticRegression())\n",
    "undersample_pipe.fit(X_train, y_train)\n",
    "probs_undersample = undersample_pipe.predict_proba(X_test)[:, 1]\n",
    "fpr_under, tpr_under, _ = roc_curve(y_test, probs_undersample)\n",
    "precision_under, recall_under, _ = precision_recall_curve(y_test, probs_undersample)\n",
    "\n",
    "lr = LogisticRegression().fit(X_train, y_train)\n",
    "probs_original = lr.predict_proba(X_test)[:, 1]\n",
    "fpr_org, tpr_org, _ = roc_curve(y_test, probs_original)\n",
    "precision_org, recall_org, _ = precision_recall_curve(y_test, probs_original)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "axes[0].plot(fpr_org, tpr_org, label=\"original\", alpha=.9)\n",
    "axes[0].plot(fpr_over, tpr_over, label=\"oversample\", alpha=.9)\n",
    "axes[0].plot(fpr_under, tpr_under, label=\"undersample\", alpha=.9)\n",
    "axes[0].legend()\n",
    "axes[0].set_xlabel(\"FPR\")\n",
    "axes[0].set_ylabel(\"TPR\")\n",
    "axes[0].set_title(\"LogReg ROC curve\")\n",
    "\n",
    "axes[1].plot(recall_org, precision_org, label=\"original\", alpha=.9)\n",
    "axes[1].plot(recall_over, precision_over, label=\"oversample\", alpha=.9)\n",
    "axes[1].plot(recall_under, precision_under, label=\"undersample\", alpha=.9)\n",
    "axes[1].legend()\n",
    "axes[1].set_xlabel(\"recall\")\n",
    "axes[1].set_ylabel(\"precision\")\n",
    "axes[1].set_title(\"LogReg PR curve\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --user shap\n",
    "# !pip install --user numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "df = pd.DataFrame(new_lr.coef_)\n",
    "ax = plt.gca()\n",
    "df.plot.bar(ax=ax, width=.9)\n",
    "ax.set_ylim(-1.5, 1.5)\n",
    "ax.set_xlim(-.5, len(df) - .5)\n",
    "ax.set_xlabel(\"feature index\")\n",
    "ax.set_ylabel(\"importance value\")\n",
    "plt.vlines(np.arange(.5, len(df) -1), -1.5, 1.5, linewidth=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "explainer = shap.LinearExplainer(new_lr, X_train, feature_dependence=\"independent\")\n",
    "shap_values = explainer.shap_values(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize the effect of all the features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shapley values calculate the importance of a feature by comparing what a model predicts with and without the feature. However, since the order in which a model sees features can affect its predictions, this is done in every possible order, so that the features are fairly compared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The y-axis indicates the variable name, in order of importance from top to bottom. The value next to them is the mean SHAP value.\n",
    "* On the x-axis is the SHAP value. Indicates how much is the change in log-odds. From this number we can extract the probability of success.\n",
    "* Gradient color indicates the original value for that variable. In booleans, it will take two colors, but in number it can contain the whole spectrum.\n",
    "* Each point represents a row from the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try this with randomforests"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
