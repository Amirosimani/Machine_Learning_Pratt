{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Lifecycle\n",
    "\n",
    "Machine learning projects are highly iterative; as you progress through the ML lifecycle, you’ll find yourself iterating on a section until reaching a satisfactory level of performance, then proceeding forward to the next task (which may be circling back to an even earlier step)\n",
    "\n",
    "[<img src=\"img/lab_05_cycle.png\"/>](https://www.jeremyjordan.me/ml-projects-guide/)\n",
    "\n",
    "\n",
    "Often times, we don't immediately know what the optimal model architecture should be for a given model, and thus we'd like to be able to explore a range of possibilities. **Start simple and gradually ramp up complexity**. This typically involves using a simple model, but can also include starting with a simpler version of your task, and data preparation.\n",
    "\n",
    "Once you have a general idea of successful model architectures and approaches for your problem, you should now spend much more focused effort on squeezing out performance gains including from the model. That's why we use **pipelines**.\n",
    "\n",
    "\n",
    "\n",
    "----\n",
    "So far, we built machine learning models and trained it on some data... now what?\n",
    "\n",
    "There are still questions that need to be answered like:\n",
    "\n",
    "* How well is my model doing? Is it a useful model?\n",
    "* Will training my model on more data improve its performance?\n",
    "* Do I need to include more features?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing & Feature Engineering\n",
    "\n",
    "\n",
    "Feature engineering asks: what is the best representation of the sample data to learn a solution to your problem?\n",
    "\n",
    "It is an art like programming is an art. Still, there are well defined procedures that are methodical, provable and understood and mainly have two goals:\n",
    "\n",
    "1. Preparing the proper input dataset, compatible with the machine learning algorithm requirements.\n",
    "2. Improving the performance of machine learning models.\n",
    "\n",
    "\n",
    "There are different techniques and steps for preprocessing and feature engineering based on the dataset and/or algorithms.\n",
    "\n",
    "Here is a list of major feature engineering techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "\n",
    "In most cases, the numerical features of the dataset do not have a certain range and they differ from each other.\n",
    "\n",
    "For example, **age** and **income** don't have the same range in real life, but from the machine learning point of view, we need a way to compare these two columns.\n",
    "\n",
    "Scaling solves this problem. The continuous features become identical in terms of the range, after a scaling process. This process is not mandatory for many algorithms, but in general, ML models benefit from scaling of the data.\n",
    "\n",
    "Remeber, if your algorithm is based on **distance** calculations such as k-NN or k-Means need to have scaled continuous features as model input.\n",
    "\n",
    "* **Normalization**:  is the process of scaling individual samples to have unit norm.\n",
    "\n",
    "X_new = (X - X_min)/(X_max - X_min)\n",
    "\n",
    "* **Standardization**: Standardization of datasets is a common requirement for many machine learning estimators implemented in scikit-learn; they might behave badly if the individual features do not more or less look like standard normally distributed data: Gaussian with **zero mean and unit variance**.\n",
    "\n",
    "X_new = (X - mean)/Std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = [[10, 0.001],\n",
    "        [8, 0.05],\n",
    "        [5, 0.005],\n",
    "        [9, 0.07],\n",
    "        [4, 0.1]]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(data)\n",
    "print(f\"Original data: {data}\")\n",
    "\n",
    "new_data = scaler.transform(data)\n",
    "print(f\"scaled data: {new_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"mean: {np.mean(new_data, axis=0)}\")\n",
    "print(f\"standard deviation: {np.std(new_data, axis=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter([item[0] for item in new_data], [item[1] for item in new_data])\n",
    "plt.scatter([item[0] for item in data], [item[1] for item in data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Imputation\n",
    "\n",
    "Missing values are one of the most common problems you can encounter when you try to prepare your data for machine learning. The reason for the missing values might be human errors, interruptions in the data flow, privacy concerns, and so on. Whatever is the reason, missing values affect the performance of the machine learning models.\n",
    "\n",
    "**Dropping missing values**: The most simple solution to the missing values is to drop the rows or the entire column.\n",
    "\n",
    "**Numerical imputation**: using Mean/Median values. This works by calculating the mean/median of the non-missing **values in** a column and then replacing the missing values within each column separately and independently from the others. It can only be used with numeric data.\n",
    "\n",
    "\n",
    "**Categorical imputation**: Replacing the missing values with the maximum occurred value in a column is a good option for handling categorical columns.\n",
    "\n",
    "The above methods don’t factor the correlations between features. It only works on the column level.\n",
    "\n",
    "**ML-based imputation** for example, **K-NN** uses ‘feature similarity’ to predict the values of any new data points. This means that the new point is assigned a value based on how closely it resembles the points in the training set. This can be very useful in making predictions about the missing values by finding the k’s closest neighbours to the observation with missing data and then imputing them based on the non-missing values in the neighbourhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp.fit([[1, 2], [np.nan, 3], [7, 6]])\n",
    "\n",
    "X = [[np.nan, 2], [6, np.nan], [7, 6]]\n",
    "print(imp.transform(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Handling outliers\n",
    "\n",
    "Many applications require being able to decide whether a new observation belongs to the same distribution as existing observations (it is an inlier), or should be considered as different (it is an outlier). Often, this ability is used to clean real data sets.. Outliers in data can have many causes such as 1-data entry error (human error), 2-Measurement errors (instrument errors), 3- Experimental errors, 4- Data processing errors, 4- Sampling errors, 5. Natural (not an error, novelties in data) and so on.\n",
    "\n",
    "Usually the best way to detect the outliers is to demonstrate the data visually (Histograms, Scatter plot, box plots, ...). However, if visualization is not possible, there are statistical methods to detect outliers such as:\n",
    "* Z-Score or Extreme Value Analysis (parametric): The z-score or standard score of an observation is a metric that indicates how many standard deviations a data point is from the sample’s mean, assuming a gaussian distribution.\n",
    "\n",
    "\n",
    "Often outliers are discarded because of their effect on the total distribution and statistical analysis of the dataset. This is certainly a good approach if the outliers are due to an error of some kind (measurement error, data corruption, etc.), however often the source of the outliers is unclear. There are many situations where occasional ‘extreme’ events cause an outlier that is outside the usual distribution of the dataset but is a valid measurement and not due to an error. In these situations, the choice of how to deal with the outliers is not necessarily clear and the choice has a significant impact on the results of any statistical analysis done on the dataset. The decision about how to deal with outliers depends on the goals and context of the research and should be detailed in any explanation about the methodology.\n",
    "\n",
    "\n",
    "There are some techniques used to deal with outliers.\n",
    "* Deleting observations\n",
    "* Transforming values: Use **robust** transformers\n",
    "* Separately treating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "#Load data\n",
    "X, y = load_boston(return_X_y=True)\n",
    "\n",
    "#Create data frame\n",
    "boston = load_boston()\n",
    "columns = boston.feature_names\n",
    "df = pd.DataFrame(X, columns = columns)\n",
    "\n",
    "ax = sns.boxplot(data=df[['CRIM', 'ZN']], orient=\"h\", palette=\"Set2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Binning\n",
    "\n",
    "The main motivation of binning is to make the model more robust and prevent overfitting, however, it has a cost to the performance. Every time you bin something, you sacrifice information and make your data more regularized.\n",
    "\n",
    "\n",
    "Binning can be applied on both categorical and numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CRIM'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KBinsDiscretizer can then be used to convert the floating values into fixed number of discrete categories with an ranked ordinal relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer  \n",
    "\n",
    "est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='quantile')\n",
    "df['CRIM_binned'] = est.fit_transform(df[['CRIM']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CRIM_binned'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way is to use `Pandas qcut`. The function defines the bins using percentiles based on the distribution of the data, not the actual numeric edges of the bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CRIM_binned_pd'] = pd.qcut(df['CRIM'], 3, labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CRIM_binned_pd'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## Encoding categorical features\n",
    "\n",
    "Often features are not given as continuous values but categorical.\n",
    "\n",
    "**OrdinalEncoder** is used to convert categorical features to such integer codes. This estimator transforms each categorical feature to one new feature of integers (0 to n_categories - 1)\n",
    "\n",
    "\n",
    "**One-hot encoding** is one of the most common encoding methods in machine learning. This method spreads the values in a column to multiple flag columns and assigns 0 or 1 to them. These binary values express the relationship between grouped and encoded column.\n",
    "\n",
    "This method changes your categorical data, which is challenging to understand for algorithms, to a numerical format and enables you to group your categorical data without losing any information. (For details please see the last part of Categorical Column Grouping)\n",
    "\n",
    "<img src=\"img/lab_05_onehot.png\"/>\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from numpy import asarray\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "\n",
    "# define data\n",
    "data = asarray([['good'], ['better'], ['best']])\n",
    "print(data)\n",
    "# define ordinal encoding\n",
    "encoder = OrdinalEncoder()\n",
    "# transform data\n",
    "result = encoder.fit_transform(data)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data\n",
    "data = asarray([['red'], ['green'], ['blue']])\n",
    "print(data)\n",
    "# define one hot encoding\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "# transform data\n",
    "onehot = encoder.fit_transform(data)\n",
    "print(onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is that this representation includes redundancy. For example, if we know that [1, 0, 0] represents “blue” and [0, 1, 0] represents “green” we don’t need another binary variable to represent “red“, instead we could use 0 values for both “blue” and “green” alone, e.g. [0, 0].\n",
    "\n",
    "This is called a **dummy variable encoding**, and always represents C categories with C-1 binary variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding/getting dummies Pandas way..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.DataFrame(data)\n",
    "print(df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(df_data,  drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Interatction/new features\n",
    "\n",
    "With tabular data, it often means a mixture of aggregating or combining features to create new features, and decomposing or splitting features to create new features.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "X = np.arange(6).reshape(3, 2)\n",
    "\n",
    "print(X)\n",
    "poly = PolynomialFeatures(2)\n",
    "poly.fit_transform(X)\n",
    "\n",
    "poly = PolynomialFeatures(interaction_only=True)\n",
    "poly.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now let's see how we can generate interaction features in a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import load_boston\n",
    "# from sklearn.linear_model import RidgeCV\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boston = load_boston()\n",
    "# X, y = boston.data, boston.target\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = RidgeCV()\n",
    "poly = PolynomialFeatures(2, interaction_only=True)\n",
    "\n",
    "pipe_rfe_poly = make_pipeline(StandardScaler(),\n",
    "                              poly,\n",
    "                              regressor\n",
    "                             )\n",
    "np.mean(cross_val_score(pipe_rfe_poly, X_train, y_train, cv=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Feature Selection\n",
    "\n",
    "Not all features are created equal. You can objectively estimate the usefulness of features. Feature selection is the process of reducing the number of input variables when developing a predictive model. \n",
    "\n",
    "\n",
    "It is desirable to reduce the number of input variables to both reduce the computational cost of modeling and, in some cases, to improve the performance of the model.\n",
    "\n",
    "Feature selection algorithms may use a scoring method to rank and choose features, such as correlation or other feature importance methods.\n",
    "\n",
    "More advanced methods may search subsets of features by trial and error, creating and evaluating models automatically in pursuit of the objectively most predictive sub-group of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "X, y = boston.data, boston.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`np.cov()` Estimate a covariance matrix, given data and weights.\n",
    "\n",
    "Covariance indicates the level to which two variables vary together. If we examine N-dimensional samples, X = [x_1, x_2, ... x_N]^T, then the covariance matrix element C_{ij} is the covariance of x_i and x_j. The element C_{ii} is the variance of x_i.\n",
    "\n",
    "\n",
    "you can also use `pd.DataFrame.corr()` to compute pairwise correlation of columns.\n",
    "\n",
    "In probability theory and statistics, the mathematical concepts of `covariance` and `correlation` are very similar. Both describe the degree to which two random variables or sets of random variables tend to deviate from their expected values in similar ways.\n",
    "\n",
    "> Covariance is when two variables vary with each other, whereas Correlation is when the change in one variable results in the change in another variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = scale(X_train)\n",
    "cov = np.cov(X_train_scaled, rowvar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8), dpi=100)\n",
    "plt.imshow(cov)\n",
    "plt.xticks(range(X.shape[1]), boston.feature_names)\n",
    "plt.yticks(range(X.shape[1]), boston.feature_names);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "useful snippet to sort values of your heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster import hierarchy\n",
    "order = np.array(hierarchy.dendrogram(hierarchy.ward(cov), no_plot=True)['ivl'], dtype=\"int\")\n",
    "\n",
    "plt.figure(figsize=(8, 8), dpi=100)\n",
    "plt.imshow(cov[order, :][:, order])\n",
    "plt.xticks(range(X.shape[1]), boston.feature_names[order])\n",
    "plt.yticks(range(X.shape[1]), boston.feature_names[order]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised feature selection\n",
    "\n",
    "`f_regression` is a univariate linear regression tests for testing individual effect of each of many regressors. \n",
    "\n",
    "This is done in 2 steps:\n",
    "1. The correlation between each regressor and the target is computed\n",
    "2. It is converted to an F score then to a p-value.\n",
    "\n",
    "\n",
    "The **F-test** of overall significance indicates whether your linear regression model provides a better fit to the data than a model that contains no independent variables.\n",
    "\n",
    "An F-test is a type of statistical test that is very flexible. You can use them in a wide variety of settings.\n",
    "\n",
    "\n",
    "A **p-value** is a measure of the probability that an observed difference could have occurred just by random chance. The lower the p-value, the greater the statistical significance of the observed difference.\n",
    "\n",
    "\n",
    "If the p-value is less than the significance level, your sample data provide sufficient evidence to conclude that your regression model fits the data better than the model with no independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_regression\n",
    "f_values, p_values = f_regression(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1)\n",
    "ax[0].set_title(\"F values\")\n",
    "ax[0].plot(f_values, 'o')\n",
    "ax[1].set_title(\"p values\")\n",
    "ax[1].plot(p_values, 'o')\n",
    "ax[1].set_yscale(\"log\")\n",
    "\n",
    "ax[1].set_xticks(range(X.shape[1]))\n",
    "ax[1].set_xticklabels(boston.feature_names, rotation=50);\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, SelectPercentile, SelectFpr\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "select = SelectKBest(k=2, score_func=f_regression)\n",
    "select.fit(X_train, y_train)\n",
    "print(X_train.shape)\n",
    "print(select.transform(X_train).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Put it all in a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "all_features = make_pipeline(StandardScaler(), RidgeCV())\n",
    "select_f = make_pipeline(StandardScaler(), \n",
    "                       SelectKBest(k=2, score_func=f_regression), \n",
    "                       RidgeCV())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy with feature selection - F test:')\n",
    "np.mean(cross_val_score(select_f, X_train, y_train, cv=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy without feature selection:')\n",
    "np.mean(cross_val_score(all_features, X_train, y_train, cv=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many more approaches for feature selection like `VarianceThreshold`, `mutual_info_regression`, and so on..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try `mutual information` and compare it with `F-Test`.\n",
    "\n",
    "**mutual information** is one of many quantities that measures how much one random variables tells us about another. It is a dimensionless quantity with (generally) units of bits, and can be thought of as the reduction in uncertainty about one random variable given knowledge of another.\n",
    "\n",
    "F-test captures only linear dependency. On the other hand, mutual information can capture any kind of dependency between variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "select_mi = make_pipeline(\n",
    "    StandardScaler(), \n",
    "    SelectKBest(k=2, score_func=mutual_info_regression), \n",
    "                       RidgeCV())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy with feature selection - mutual information:')\n",
    "np.mean(cross_val_score(select_mi, X_train, y_train, cv=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare MI score and F values\n",
    "\n",
    "scores = mutual_info_regression(X_train, y_train)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 2))\n",
    "line_f, = plt.plot(f_values, 'o', c='r')\n",
    "plt.ylabel(\"F value\")\n",
    "ax2 = plt.twinx()\n",
    "line_s, = ax2.plot(scores, 'o', alpha=.7)\n",
    "ax2.set_ylabel(\"MI score\")\n",
    "plt.xticks(range(X.shape[1]), boston.feature_names)\n",
    "plt.legend([line_s, line_f], [\"Mutual info scores\", \"F values\"], loc=(0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive feature elimination\n",
    "\n",
    "recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features.That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "regressor = RidgeCV()\n",
    "rfecv = RFECV(estimator=regressor, step=1, cv=5,\n",
    "              scoring='r2')\n",
    "\n",
    "rfecv_pipe = make_pipeline(StandardScaler(),\n",
    "                         rfecv)\n",
    "\n",
    "rfecv_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimal number of features : %d\" % rfecv_pipe['rfecv'].n_features_)\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (r2)\")\n",
    "plt.plot(range(1, len(rfecv_pipe['rfecv'].grid_scores_) + 1), rfecv_pipe['rfecv'].grid_scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note: 𝑅2  is negative only when the chosen model does not follow the trend of the data, so fits worse than a horizontal line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remeber Lasso regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "\n",
    "lasso_pipe = make_pipeline(StandardScaler(),\n",
    "                         LassoCV())\n",
    "lasso_pipe.fit(X_train, y_train)\n",
    "\n",
    "print(lasso_pipe['lassocv'].coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 2))\n",
    "line_f, = plt.plot(f_values, 'o', c='r')\n",
    "plt.ylabel(\"F value\")\n",
    "ax2 = plt.twinx()\n",
    "ax2.set_ylabel(\"lasso coefficients\")\n",
    "line_s, = ax2.plot(np.abs(lasso_pipe['lassocv'].coef_), 'o', alpha=.7)\n",
    "plt.xticks(range(X.shape[1]), boston.feature_names)\n",
    "plt.legend([line_s, line_f], [\"Lasso coefficients abs\", \"F values\"], loc=(0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "A model **hyperparameter** is a configuration that is external to the model and whose value cannot be estimated from data.\n",
    "\n",
    "* They are often used in processes to help estimate model parameters.\n",
    "* They are often specified by the practitioner.\n",
    "* They can often be set using heuristics.\n",
    "* They are often tuned for a given predictive modeling problem.\n",
    "\n",
    "Hyperparameters address model design questions such as:\n",
    "* K in K-NN\n",
    "* What should be the maximum depth allowed for my decision tree?\n",
    "* How many trees should I include in my random forest?\n",
    "* How many layers should I have in my neural network?\n",
    "\n",
    "\n",
    "We cannot know the best value for a model hyperparameter on a given problem. We may use rules of thumb, copy values used on other problems, or search for the best value by trial and error.\n",
    "\n",
    "[<img src=\"img/lab_06_hpt.png\"/>](https://www.analyticsvidhya.com/blog/2021/04/evaluating-machine-learning-models-hyperparameter-tuning/)\n",
    "\n",
    "-----\n",
    "Searching for the best hyper-parameter can be tedious, hence search algorithms like **grid search** or **random search**. USing these, you are tuning the hyperparameters of the model in order to discover the parameters of the model that result in the most skillful predictions.\n",
    "\n",
    "Use **coarse-to-fine random searches** for hyperparameters. Start with a wide hyperparameter space initially and iteratively hone in on the highest-performing region of the hyperparameter space.\n",
    "\n",
    "Please note that optimal hyperparameters often differ for different datasets and models.\n",
    "\n",
    "A search consists of:\n",
    "\n",
    "* an estimator (regressor or classifier such as sklearn.svm.SVC());\n",
    "* a parameter space;\n",
    "* a method for searching or sampling candidates;\n",
    "* a cross-validation scheme; and\n",
    "* a score function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.data, data.target, stratify=data.target, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a pipeline to search for the best combination of PCA truncation\n",
    "# and classifier regularization.\n",
    "\n",
    "# set the tolerance to a large value to make the example faster\n",
    "step_1 = StandardScaler()\n",
    "model_1 = LogisticRegression(max_iter=100, tol=0.1)\n",
    "\n",
    "pipe_1 = Pipeline(steps=[('custom_scaler', step_1),\n",
    "                         ('logreg_1', model_1)])\n",
    "\n",
    "# Parameters of pipelines can be set using ‘__’ separated parameter names:\n",
    "param_grid = {\n",
    "    'logreg_1__C': np.logspace(-4, 4, 4),\n",
    "    'logreg_1__penalty':['l2', 'l1'],\n",
    "    'logreg_1__class_weight':['balanced', 'none']\n",
    "    \n",
    "}\n",
    "search = GridSearchCV(pipe_1, \n",
    "                      param_grid,\n",
    "                      scoring='f1_macro',\n",
    "                      cv=10,\n",
    "                      n_jobs=-1\n",
    "                     )\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = search.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_lr = search.best_estimator_['logreg']\n",
    "new_lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = new_lr.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "Establish performance baselines on your problem. Baselines are useful for both establishing a lower bound of expected performance (simple model baseline) and establishing a target performance level (human baseline).\n",
    "\n",
    "* Simple baselines include out-of-the-box scikit-learn models (i.e. logistic regression with default parameters) or even simple heuristics (always predict the majority class). Without these baselines, it's impossible to evaluate the value of added model complexity.\n",
    "* If your problem is well-studied, search the literature to approximate a baseline based on published results for very similar tasks/datasets.\n",
    "* If possible, try to estimate human-level performance on the given task. Don't naively assume that humans will perform the task perfectly, a lot of simple tasks are deceptively hard!\n",
    "\n",
    "## Why is my model performing poorly?\n",
    "\n",
    "There a many reasons why your model is not performing as you like it.\n",
    "\n",
    "* Implementation bugs\n",
    "* Hyperparameter choices\n",
    "* Data/model fit\n",
    "* Dataset construction\n",
    "\n",
    "\n",
    "### Train/Test split\n",
    "\n",
    "First step to properly evaluate your model is to not train the model on the entire dataset. I repeat: **do not train the model on the entire dataset.**\n",
    "\n",
    "\n",
    "<img src=\"img/lab_05_split.png\"/>\n",
    "\n",
    "The training set contains a known output and the model learns on this data in order to be generalized to other data later on. We have the test dataset (or subset) in order to test our model’s prediction on this subset.\n",
    "\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "There are a variety of metrics you can use to evaluate your regression, clssification, and clustering models. You can read about many of them [here](https://scikit-learn.org/stable/modules/model_evaluation.html)\n",
    "\n",
    "### Regression metrics\n",
    "\n",
    "There are three error metrics that are commonly used for evaluating and reporting the performance of a regression model; they are:\n",
    "\n",
    "* Mean Squared Error (MSE).\n",
    "\n",
    "<img src=\"img/lab_06_mse.png\" style=\"width: 200px;\"/>\n",
    "\n",
    "The squaring  has the effect of inflating or magnifying large errors. That is, the larger the difference between the predicted and expected values, the larger the resulting squared positive error. This has the effect of “punishing” models more for larger errors when MSE is used as a loss function. It also has the effect of “punishing” models by inflating the average error score when used as a metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of increase in mean squared error\n",
    "from matplotlib import pyplot\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# real value\n",
    "expected = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
    "# predicted value\n",
    "predicted = [1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.0]\n",
    "# calculate errors\n",
    "errors = list()\n",
    "for i in range(len(expected)):\n",
    "    # calculate error\n",
    "    err = mean_squared_error([expected[i]], [predicted[i]])\n",
    "    # store error\n",
    "    errors.append(err)\n",
    "    print('>%.1f, %.1f = %.3f' % (expected[i], predicted[i], err))\n",
    "# plot errors\n",
    "pyplot.plot(errors)\n",
    "pyplot.xticks(ticks=[i for i in range(len(errors))], labels=predicted)\n",
    "pyplot.xlabel('Predicted Value')\n",
    "pyplot.ylabel('Mean Squared Error')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "* Root Mean Squared Error (RMSE): is an extension of the mean squared error.\n",
    "\n",
    "Importantly, the square root of the error is calculated, which means that the units of the RMSE are the same as the original units of the target value that is being predicted. For example, if your target variable has the units “dollars,” then the RMSE error score will also have the unit “dollars” and not “squared dollars” like the MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(expected, predicted, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "* Mean Absolute Error (MAE): is a popular metric because, like RMSE, the units of the error score match the units of the target value that is being predicted. Unlike the RMSE, the changes in MAE are linear and therefore intuitive.\n",
    "\n",
    "<img src=\"img/lab_06_mae.png\" style=\"width: 200px;\"/>\n",
    "\n",
    "As its name suggests, the MAE score is calculated as the average of the absolute error values. Absolute or abs() is a mathematical function that simply makes a number positive. Therefore, the difference between an expected and predicted value may be positive or negative and is forced to be positive when calculating the MAE.\n",
    "\n",
    "The main problem with the F1 score is that it gives equal weight to precision and recall. We might sometimes need to include domain knowledge in our evaluation where we want to have more recall or more precision.\n",
    "\n",
    "\n",
    "* **R Squared / Coefficient of Determination**\n",
    "\n",
    "is a statistical measure that represents the goodness of fit of a regression model. The ideal value for r-square is 1. The closer the value of r-square to 1, the better is the model fitted.\n",
    "\n",
    "R-square is a comparison of residual sum of squares (SSres) with total sum of squares(SStot). Total sum of squares is calculated by summation of squares of perpendicular distance between data points and the average line.\n",
    "\n",
    "A residual is a measure of how far away a point is vertically from the regression line. Simply, it is the error between a predicted value and the observed actual value.\n",
    "\n",
    "<img src=\"img/lab_06_r2.jpeg\" style=\"width: 600px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Residual plots\n",
    "\n",
    "A typical residual plot has the residual values on the Y-axis and the independent variable on the x-axis. The figure below is a good example of how a typical residual plot looks like.\n",
    "\n",
    "<img src=\"img/lab_06_residual.png\"/>\n",
    "\n",
    "Every regression model inherently has some degree of error since you can never predict something 100% accurately. More importantly, randomness and unpredictability are always a part of the regression model. Hence, a regression model can be explained as:\n",
    "\n",
    "`Response = Deterministic + Stochastic`\n",
    "\n",
    "The deterministic part of the model is what we try to capture using the regression model. Ideally, our linear equation model should accurately capture the predictive information. Essentially, what this means is that if we capture all of the predictive information, all that is left behind (residuals) should be completely random & unpredictable i.e stochastic. Hence, we want our residuals to follow a normal distribution.\n",
    "\n",
    "<img src=\"img/lab_06_residual_projected.png\"/>\n",
    "\n",
    "**In a good residual plot, we do not see any patterns in the value of the residuals as we move along the x-axis.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Classification metrics\n",
    "\n",
    "\n",
    "<img src=\"img/lab_06_tf_tn.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "* **Accuracy**\n",
    "\n",
    "Accuracy is the quintessential classification metric. It is pretty easy to understand. And easily suited for binary as well as a multiclass classification problem.\n",
    "\n",
    "`Accuracy = (TP+TN)/(TP+FP+FN+TN)`\n",
    "\n",
    "Accuracy is the proportion of true results among the total number of cases examined.\n",
    "\n",
    "When the target class is very sparse (think rare events like if an astroid will hit the earth), accuracy is not a good choice.\n",
    "\n",
    "* **Precision**\n",
    "\n",
    "Precision tells us the the proportion of predicted Positives is truly Positive\n",
    "\n",
    "`Precision = (TP)/(TP+FP)`\n",
    "\n",
    "In the asteroid prediction problem, we never predicted a true positive.\n",
    "And thus precision=0\n",
    "\n",
    "Precision is a good evaluation metric to use when the **cost of a false positive is very high and the cost of a false negative is low**. For example, precision is good to use if you are a restaurant owner looking to buy wine for your restaurant only if it is predicted to be good by a classifier algorithm.\n",
    "\n",
    "* **Recall**\n",
    " \n",
    " Recall answers a different question: what proportion of actual Positives is correctly classified?\n",
    "\n",
    "`Recall = (TP)/(TP+FN)`\n",
    "\n",
    "In the asteroid prediction problem, we never predicted a true positive.\n",
    "And thus recall is also equal to 0.\n",
    "\n",
    "Recall is a valid choice of evaluation metric when we want to capture as many positives as possible. For example: If we are building a system to predict if a person has cancer or not, we want to capture the disease even if we are not very sure.\n",
    "\n",
    "However having a high recall doesn’t necessary mean a model is good. For example, if a model predicted that everyone had a disease, the model would have a perfect recall but it would have a lot of false positives and be telling people they were sick when they were not.\n",
    "\n",
    "\n",
    "<img src=\"img/lab_06_pr.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "* **F1 Score**\n",
    "\n",
    "The F1 score is a number between 0 and 1 and is the harmonic mean of precision and recall.\n",
    "\n",
    "<img src=\"img/lab_06_f1.png\"/>\n",
    "\n",
    "\n",
    "F1 score maintains a balance between the precision and recall for your classifier. If your precision is low, the F1 is low and if the recall is low again your F1 score is low.\n",
    "\n",
    "\n",
    "----\n",
    "There are more classifacation metrics like Log Loss and Binary Crossentropy which we will see in Neural Networks section of the course.\n",
    "\n",
    "\n",
    "\n",
    "#### Beware not to use a regression scoring function with a classification problem, you will get useless results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "data = load_breast_cancer()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.data, data.target, stratify=data.target, random_state=0)\n",
    "\n",
    "lr = LogisticRegression().fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# print(confusion_matrix(y_test, y_pred))\n",
    "print(lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "----\n",
    "### Precision Recall Curve\n",
    "\n",
    "\n",
    "The Precision-Recall curves is used for evaluating the performance of binary classification algorithms. It summarize the trade-off between the true positive rate and the positive predictive value for a predictive model using different probability **thresholds**.\n",
    "\n",
    "For example, if we use logistic regression, the threshold would be the predicted probability of an observation belonging to the positive class. Normally in logistic regression, if an observation is predicted to belong to the positive class at probability > 0.5, it is labeled as positive. However, we could really choose any probability threshold between 0 and 1. A precision-recall curve helps to visualize how the choice of threshold affects classifier performance, and can even help us select the best threshold for a specific problem.\n",
    "\n",
    "Precision-Recall is a useful measure of success of prediction when the classes are very **imbalanced**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Add noisy features\n",
    "random_state = np.random.RandomState(0)\n",
    "n_samples, n_features = X.shape\n",
    "X = np.c_[X, random_state.randn(n_samples, 200 * n_features)]\n",
    "\n",
    "# Limit to the two first classes, and split into training and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[y < 2], y[y < 2],\n",
    "                                                    test_size=.5,\n",
    "                                                    random_state=random_state)\n",
    "\n",
    "# Create a simple classifier\n",
    "classifier = svm.LinearSVC(random_state=random_state)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_score = classifier.decision_function(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "disp = plot_precision_recall_curve(classifier, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can summarize the information in a precision-recall curve with a single value. This summary metric is the **AUC-PR**. AUC-PR stands for area under the (precision-recall) curve. Generally, the higher the AUC-PR score, the better a classifier performs for the given task.\n",
    "\n",
    "In a perfect classifier, AUC-PR =1. In a “baseline” classifier, the AUC-PR will depend on the fraction of observations belonging to the positive class. For example, in a balanced binary classification data set, the “baseline” classifier will have AUC-PR = 0.5. A classifier that provides some predictive value will fall between the “baseline” and perfect classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### AUC-ROC Curve\n",
    "\n",
    "AUC ROC indicates how well the probabilities from the positive classes are separated from the negative classes.\n",
    "\n",
    "ROC Curves summarize the trade-off between the true positive rate and false positive rate for a predictive model using different probability thresholds.\n",
    "\n",
    "Where True positive rate or TPR is just the proportion of trues we are capturing using our algorithm.\n",
    "`Sensitivty = TPR(True Positive Rate)= Recall = TP/(TP+FN)`\n",
    "\n",
    "and False positive rate or FPR is just the proportion of false we are capturing using our algorithm.\n",
    "\n",
    "`1- Specificity = FPR(False Positive Rate)= FP/(TN+FP)`\n",
    "\n",
    "\n",
    "\n",
    "A ROC curve is constructed by plotting the true positive rate (TPR) against the false positive rate (FPR).\n",
    "\n",
    "Is appropriate when the observations are balanced between each class\n",
    " As a baseline, a random classifier is expected to give points lying along the diagonal (FPR = TPR). The closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test.\n",
    " \n",
    " \n",
    " we can use the ROC curves to decide on a Threshold value.\n",
    "The choice of threshold value will also depend on how the classifier is intended to be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever you apply a classifier to assign a label against an observation, the classifier generates a probability against the observation and not the label. The probability is the indicator of how confidently you can assign a label against the observation and then after comparing it with a preset threshold value you assign the label to it. If you relax your threshold to a lower value, your test observations will have more number of readers labeled as Yes. The controlling threshold depends on the use case. For example, in the advertisement industry, your goal is to capture the maximum number of people who will click on the ad. Therefore, you can relax your threshold while predicting so that you can target more people.\n",
    "\n",
    "ROC or Receiver Operator Characteristic curve is a plot of the Recall (True Positive Rate) (on the y-axis) versus the False Positive Rate (on the x-axis) for every possible classification threshold.\n",
    "\n",
    "The reason why you should check the ROC curve to evaluate a classifier instead of a simpler metric such as accuracy is that a ROC curve visualizes all possible classification thresholds, whereas accuracy only represents performance for a single threshold. Typical ROC curve looks like the image shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roc curve and auc\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot\n",
    "\n",
    "\n",
    "# generate 2 class dataset\n",
    "X, y = make_classification(n_samples=1000, n_classes=2, random_state=1)\n",
    "# split into train/test sets\n",
    "trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.5, random_state=2)\n",
    "# generate a no skill prediction (majority class)\n",
    "ns_probs = [0 for _ in range(len(testy))]\n",
    "# fit a model\n",
    "model = LogisticRegression(solver='lbfgs')\n",
    "model.fit(trainX, trainy)\n",
    "# predict probabilities\n",
    "lr_probs = model.predict_proba(testX)\n",
    "# keep probabilities for the positive outcome only\n",
    "lr_probs = lr_probs[:, 1]\n",
    "# calculate scores\n",
    "ns_auc = roc_auc_score(testy, ns_probs)\n",
    "lr_auc = roc_auc_score(testy, lr_probs)\n",
    "# summarize scores\n",
    "print('No Skill: ROC AUC=%.3f' % (ns_auc))\n",
    "print('Logistic: ROC AUC=%.3f' % (lr_auc))\n",
    "# calculate roc curves\n",
    "ns_fpr, ns_tpr, _ = roc_curve(testy, ns_probs)\n",
    "lr_fpr, lr_tpr, _ = roc_curve(testy, lr_probs)\n",
    "# plot the roc curve for the model\n",
    "pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "pyplot.plot(lr_fpr, lr_tpr, marker='.', label='Logistic')\n",
    "# axis labels\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Imbalanced data\n",
    "\n",
    "\n",
    "As discussed, imbalanced classification problem is an example of a classification problem where the distribution of examples across the known classes is biased or skewed. The distribution can vary from a slight bias to a severe imbalance where there is one example in the minority class for hundreds, thousands, or millions of examples in the majority class or classes.\n",
    "\n",
    "Imbalanced classifications pose a challenge for predictive modeling as most of the machine learning algorithms used for classification were designed around the assumption of an equal number of examples for each class.\n",
    "\n",
    "This results in models that have poor predictive performance, specifically for the minority class. This is a problem because typically, the minority class is more important and therefore the problem is more sensitive to classification errors for the minority class than the majority class.\n",
    "\n",
    "There are techniques to handle imbalanced data, including:\n",
    "\n",
    "* Collecting more data!\n",
    "* Using the right evaluation metrics: F1 vs Accuracy\n",
    "* Resampling training set\n",
    "    - undersampling: balances the dataset by reducing the size of the abundant class. This method is used when quantity of data is sufficient. \n",
    "    - oversampling: oversampling is used when the quantity of data is insufficient. It tries to balance dataset by increasing the size of rare samples. \n",
    "* Trying different algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from imblearn.datasets import make_imbalance\n",
    "from imblearn.under_sampling import NearMiss, RandomUnderSampler\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Create a folder to fetch the dataset\n",
    "iris = load_iris()\n",
    "X, y = make_imbalance(\n",
    "    iris.data,\n",
    "    iris.target,\n",
    "    sampling_strategy={0: 25, 1: 50, 2: 50},\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=RANDOM_STATE)\n",
    "\n",
    "print(f\"Training target statistics: {Counter(y_train)}\")\n",
    "print(f\"Testing target statistics: {Counter(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline WITHOUT resampling\n",
    "pipeline = make_pipeline(\n",
    "    StandardScaler(), \n",
    "    LogisticRegression(random_state=RANDOM_STATE)\n",
    ")\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Classify and report the results\n",
    "print(classification_report_imbalanced(y_test, pipeline.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline WITH resampling\n",
    "pipeline = make_pipeline(\n",
    "    RandomUnderSampler(sampling_strategy='auto', random_state=42),\n",
    "    StandardScaler(), \n",
    "    LogisticRegression(random_state=RANDOM_STATE)\n",
    ")\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Classify and report the results\n",
    "print(classification_report_imbalanced(y_test, pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Clustering metrics\n",
    "\n",
    "\n",
    "In clustering you don’t have any labels, just a set of features for observation and your goal is to create clusters that have similar observations clubbed together and dissimilar observations kept as far as possible.\n",
    "\n",
    "Evaluating the performance of a clustering algorithm is not as trivial as counting the number of errors or the precision and recall like in the case of supervised learning algorithms.\n",
    "\n",
    "Here clusters are evaluated based on some similarity or dissimilarity measure such as the distance between cluster points. If the clustering algorithm separates dissimilar observations apart and similar observations together, then it has performed well. The One of the most popular metrics evaluation metrics for clustering algorithms is the **Silhouette coefficient**.\n",
    "\n",
    "The Silhouette Coefficient is defined for each sample and is composed of two scores:\n",
    "* a: The mean distance between a sample and all other points in the same cluster.\n",
    "* b: The mean distance between a sample and all other points in the next nearest cluster.\n",
    "\n",
    "<img src=\"img/lab_06_sc.png\"/>\n",
    "\n",
    "\n",
    "The Silhouette Coefficient for a set of samples is given as the mean of the Silhouette Coefficient for each sample. The score is bounded between -1 for incorrect clustering and +1 for highly dense clustering. Scores around zero indicate overlapping clusters. The score is higher when clusters are dense and well separated, which relates to a standard concept of a cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check sklearn for an example.\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
