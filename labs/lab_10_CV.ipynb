{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn scikit-learn tensorflow pooch -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers, models, datasets\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"savefig.dpi\"] = 300\n",
    "plt.rcParams[\"savefig.bbox\"] = \"tight\"\n",
    "# Make numpy values easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image as data\n",
    "Sofar we've seen that everytime we work with ML models, we need to have the input data as vectors. There’s almost no ML model where vectors aren’t used at some point in the project lifecycle. Machines can’t read text or look at images like you and me. They need input to be transformed or encoded into numbers. \n",
    "\n",
    "Please be aware that the a strictly mathematical definition of vectors can fail to convey all the information you need to work with and understand vectors in an ML context like this:\n",
    "\n",
    "<img src=\"img/lab_10_img_as_data_.png\"/>\n",
    "\n",
    "so for MNIST data, we turned a (28, 28) image to a vector of size (748,).\n",
    "\n",
    "An image is nothing but a matrix of pixel values. In last session, we just flattened them in order to train a sequential NN on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train),(x_test, y_test) = datasets.mnist.load_data()\n",
    "\n",
    "# rescale the images from [0, 255] to the [0.0, 0.1] range\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "print(\"Number of original training examples:\", len(x_train))\n",
    "print(\"Number of original test examples:\", len(x_test))\n",
    "\n",
    "# check the first 10 examples\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(10):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(x_train[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[0].shape, x_train[0].flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,5, figsize=(15, 6), facecolor='w', edgecolor='k')\n",
    "fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "\n",
    "# axs = axs.ravel()\n",
    "\n",
    "for i in range(5):\n",
    "    ar = x_train[i].flatten()\n",
    "    axs[0, i].bar(np.arange(len(ar)), ar)\n",
    "    axs[1, i].imshow(x_train[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "\n",
    "\n",
    "A **Convolutional Neural Network** (ConvNet/CNN) is a Deep Learning algorithm which can take in an input image, assign importance (learnable weights and biases) to various aspects/objects in the image and be able to differentiate one from the other.\n",
    "\n",
    "\n",
    "The architecture of a ConvNet is analogous to that of the connectivity pattern of Neurons in the Human Brain and was inspired by the organization of the Visual Cortex. Individual neurons respond to stimuli only in a restricted region of the visual field known as the Receptive Field. A collection of such fields overlap to cover the entire visual area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why CNN as oppose to sequential models?\n",
    "You can always feed the flatten images to a sequential model, however a ConvNet is able to **successfully capture the Spatial and Temporal dependencies** in an image through the application of relevant filters. \n",
    "\n",
    "In the figure, we have an RGB image which has been separated by its three color planes — Red, Green, and Blue.\n",
    "\n",
    "<img src=\"img/lab_10_rgb.png\"/>\n",
    "\n",
    "You can imagine how computationally intensive things would get once the images reach dimensions, say 8K (7680×4320). The role of the ConvNet is to reduce the images into a form which is easier to process, without losing features which are critical for getting a good prediction. This is important when we are to design an architecture which is not only good at learning features but also is scalable to massive datasets.\n",
    "\n",
    "The architecture performs a better fitting to the image dataset due to the reduction in the number of parameters involved and reusability of weights. In other words, the network can be trained to understand the sophistication of the image better.\n",
    "\n",
    "\n",
    "## Convolution Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(url=\"https://miro.medium.com/max/1000/1*GcI7G-JLAQiEoCON7xFbhg.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**local receptive fields**: In a typical neural network, each neuron in the input layer is connected to a neuron in the hidden layer. However, in a CNN, only a small region of input layer neurons connects to neurons in the hidden layer. These regions are referred to as **local receptive fields**. You can use **Convolusion** to implement this process efficiently. \n",
    "\n",
    "The element involved in carrying out the convolution operation in the first part of a Convolutional Layer is called the **Kernel/Filter**, K, represented in the color yellow. We have selected K as a 3x3x1 matrix.\n",
    "\n",
    "\n",
    "The Kernel shifts 9 times because of **Stride Length** = 1 (Non-Strided), every time performing a matrix multiplication operation between K and the portion P of the image over which the kernel is hovering.\n",
    "\n",
    "The objective of the Convolution Operation is to extract the high-level features such as edges, from the input image. \n",
    "\n",
    "ConvNets need not be limited to only one Convolutional Layer. \n",
    "\n",
    "The first ConvLayer is responsible for capturing the Low-Level features such as edges, color, gradient orientation, etc. With added layers, the architecture adapts to the High-Level features as well, giving us a network which has the wholesome understanding of images in the dataset, similar to how we would."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load 'ascent' image from scipy\n",
    "i = scipy.datasets.ascent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.grid(False)\n",
    "plt.gray()\n",
    "plt.axis('off')\n",
    "plt.imshow(i)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_transformed = np.copy(i)\n",
    "# Get the dimensions of the image to loop over it later\n",
    "size_x = i_transformed.shape[0]\n",
    "size_y = i_transformed.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a 3x3 filter\n",
    "\n",
    "# Experiment with different values for fun effects.\n",
    "filter = [ [-1, 0, 1], [-1, 0, 1], [-1, 0, 1]]\n",
    "filter = [ [1, 0, 0], [0, 1, 0], [0, 0, 1]]\n",
    "filter = [ [0, 0, 1], [0, 1, 0], [1, 0, 0]]\n",
    "\n",
    "\n",
    "weight  = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's creata convolution. Notice in the code, you iterate over the image, \n",
    "# leaving a 1 pixel margin, and multiply out each of the neighbors of the current pixel\n",
    "#  by the value defined in the filter.\n",
    "\n",
    "for x in range(1,size_x-1):\n",
    "  for y in range(1,size_y-1):\n",
    "      convolution = 0.0\n",
    "      convolution = convolution + (i[x - 1, y-1] * filter[0][0])\n",
    "      convolution = convolution + (i[x, y-1] * filter[0][1])\n",
    "      convolution = convolution + (i[x + 1, y-1] * filter[0][2])\n",
    "      convolution = convolution + (i[x-1, y] * filter[1][0])\n",
    "      convolution = convolution + (i[x, y] * filter[1][1])\n",
    "      convolution = convolution + (i[x+1, y] * filter[1][2])\n",
    "      convolution = convolution + (i[x-1, y+1] * filter[2][0])\n",
    "      convolution = convolution + (i[x, y+1] * filter[2][1])\n",
    "      convolution = convolution + (i[x+1, y+1] * filter[2][2])\n",
    "      convolution = convolution * weight\n",
    "      if(convolution<0):\n",
    "        convolution=0\n",
    "      if(convolution>255):\n",
    "        convolution=255\n",
    "      i_transformed[x, y] = convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the image. Note the size of the axes -- they are 512 by 512\n",
    "plt.gray()\n",
    "plt.grid(False)\n",
    "plt.imshow(i_transformed)\n",
    "#plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Pooling Layer\n",
    "\n",
    "Similar to the Convolutional Layer, the Pooling layer is responsible for reducing the overall amount of information in an image while maintaining the features that are detected as present.\n",
    "\n",
    "\n",
    "\n",
    "Pooling reduces the dimensionality of the featured map by condensing the output of small regions of neurons into a single output. This helps simplify the following layers and reduces the number of parameters that the model needs to learn.\n",
    "\n",
    "\n",
    "\n",
    "There are two common types of Pooling: Max Pooling and Average Pooling. Max Pooling returns the maximum value from the portion of the image covered by the Kernel. On the other hand, Average Pooling returns the average of all the values from the portion of the image covered by the Kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url=\"https://miro.medium.com/max/792/1*uoWYsCV5vBU8SHFPAPao-w.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "After going through the above process (kernel and max pooling), we have successfully enabled the model to understand the features. Moving on, we are going to flatten the final output and feed it to a regular Neural Network for classification purposes.\n",
    "\n",
    "Adding a Fully-Connected layer is a (usually) cheap way of learning non-linear combinations of the high-level features as represented by the output of the convolutional layer. The Fully-Connected layer is learning a possibly non-linear function in that space.\n",
    "\n",
    "\n",
    "Now that we have converted our input image into a suitable form for our Multi-Level Perceptron, we flatten the image into a column vector. The flattened output is fed to a feed-forward neural network and backpropagation applied to every iteration of training. Over a series of epochs, the model is able to distinguish between dominating and certain low-level features in images and classify them using the Softmax Classification technique.\n",
    "\n",
    "<img src=\"img/lab_10_cnn_2.png\"/>\n",
    "\n",
    "To summarize, A convolution is a filter that passes over an image, processes it, and extracts features that show a commonality in the image.\n",
    "\n",
    "The process is simple. You scan every pixel in the image and then look at its neighboring pixels. You multiply out the values of these pixels by equivalent weights in a filter.\n",
    "\n",
    "<img src=\"img/lab_10_cnn.png\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify MNIST data using CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape dataset to have a single channel\n",
    "x_train = x_train.reshape((x_train.shape[0], 28, 28, 1))\n",
    "x_test = x_test.reshape((x_test.shape[0], 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode target values\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the convolutional base\n",
    "\n",
    "The 6 lines of code below define the convolutional base using a common pattern: a stack of Conv2D and MaxPooling2D layers.\n",
    "\n",
    "As input, a CNN takes tensors of shape (image_height, image_width, color_channels), ignoring the batch size. If you are new to these dimensions, color_channels refers to (R,G,B). In this example, you will configure your CNN to process inputs of shape (32, 32, 3), which is the format of CIFAR images. You can do this by passing the argument input_shape to your first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can see that the output of every Conv2D and MaxPooling2D layer is a 3D tensor of shape (height, width, channels). The width and height dimensions tend to shrink as you go deeper in the network. The number of output channels for each Conv2D layer is controlled by the first argument (e.g., 32 or 64). Typically, as the width and height shrink, you can afford (computationally) to add more output channels in each Conv2D layer.\n",
    "\n",
    "----\n",
    "\n",
    "To complete the model, you will feed the last output tensor from the convolutional base (of shape (4, 4, 64)) into one or more Dense layers to perform classification. Dense layers take vectors as input (which are 1D), while the current output is a 3D tensor. First, you will flatten (or unroll) the 3D output to 1D, then add one or more Dense layers on top. CIFAR has 10 output classes, so you use a final Dense layer with 10 outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy',\n",
    "              tf.keras.metrics.F1Score(average='macro')])\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=10, \n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['f1_score'], label='training f1')\n",
    "plt.plot(history.history['val_f1_score'], label = 'validation f1')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('F1 score')\n",
    "plt.ylim([0.9, 1])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "test_loss, test_acc, test_f1 = model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='training loss')\n",
    "plt.plot(history.history['val_loss'], label = 'validation loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim([0, 0.2])\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "test_loss, test_acc, test_f1 = model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "please note that the validation loss is somewhat increasing while the training loss is decreasing. This is a case of overfitting. The training loss will always tend to improve as training continues up until the model's capacity to learn has been saturated. When training loss decreases but validation loss increases, your model has reached the point where it has stopped learning the general problem and started learning the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test, batch_size=64, \n",
    "                               verbose=1)\n",
    "y_pred_bool = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(classification_report(np.argmax(y_test, axis=1), y_pred_bool))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_1 = model.layers[0]\n",
    "weights, biases = layer_1.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 8)\n",
    "for ax, weight in zip(axes.ravel(), weights.T):\n",
    "    ax.imshow(weight[0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_2 = model.layers[2]\n",
    "weights, biases = layer_2.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 8)\n",
    "for ax, weight in zip(axes.ravel(), weights.T):\n",
    "    ax.imshow(weight[0, :, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various architectures of CNNs available. Some of them have been listed below:\n",
    "\n",
    "* AlexNet (5 convolutional layers, 3 max-pooling layers, 2 normalization layers, 2 fully connected layers, and 1 softmax layer)\n",
    "* VGGNet (up to 19 layers)\n",
    "* GoogLeNet (22 layers deep, with 27 pooling layers included)\n",
    "* ResNet (34, 50, 101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch normalization is a technique for training very deep neural networks that standardizes the inputs to a layer for each mini-batch. This normalization step is applied right before (or right after) the nonlinear function.\n",
    "\n",
    "This has the effect of stabilizing the learning process and dramatically reducing the number of training epochs required to train deep networks.\n",
    "\n",
    "\n",
    "* Deep neural networks are challenging to train, not least because the input from prior layers can change after weight updates.\n",
    "* Batch normalization is a technique to standardize the inputs to a network, applied to either the activations of a prior layer or inputs directly.\n",
    "* Batch normalization accelerates training, in some cases by halving the epochs or better, and provides some regularization, reducing generalization error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import  BatchNormalization\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')   \n",
    "])\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and train the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy',\n",
    "                       tf.keras.metrics.F1Score(average='macro')])\n",
    "\n",
    "\n",
    "historybn = model.fit(x_train, y_train, epochs=5, \n",
    "                      validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(historybn.history['f1_score'], label='training f1')\n",
    "plt.plot(historybn.history['val_f1_score'], label = 'validation f1')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('F1 score')\n",
    "plt.ylim([0.9, 1])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "test_loss, test_acc, test_f1 = model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(historybn.history['loss'], label='training loss')\n",
    "plt.plot(historybn.history['val_loss'], label = 'validation loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim([0, 0.2])\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "test_loss, test_acc, test_f1 = model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test, batch_size=64, \n",
    "                               verbose=1)\n",
    "y_pred_bool = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(classification_report(np.argmax(y_test, axis=1), y_pred_bool))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
