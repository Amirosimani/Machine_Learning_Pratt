{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "Unsupervised learning subsumes all kinds of machine learning where there is no known output, no teacher to instruct the learning algorithm. In unsupervised learning, the learning algorithm is just shown the input data and asked to extract knowledge from this data.\n",
    "\n",
    "\n",
    "In the supervised learning setting, we typically have access to a set of p features <em>X1,X2,...,Xp</em>, measured on n observations, and a response Y also measured on those same n observations. The goal is then to predict Y using <em>X1,X2,...,Xp</em>.\n",
    "\n",
    "Unsupervised learning is a set of statistical tools intended for the setting in which we have only a set of features <em>X1, X2,..., Xp</em> measured on n observations. **We are not interested in prediction**, because we do not have an associated response variable Y . \n",
    "\n",
    "Rather, the goal is to discover interesting things about the measurements on X1, X2, . . . , Xp. Is there an informative way to visualize the data? Can we discover subgroups among the variables or among the observations? Unsupervised learning refers to a diverse set of techniques for answering questions such as these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "\n",
    "Clustering refers to a very broad set of techniques for finding subgroups, or clusters, in a data set. \n",
    "\n",
    "When we cluster the observations of a data set, we seek to partition them into distinct groups so that the observations within each group are quite similar to each other, while observations in different groups are quite different from each other. \n",
    "\n",
    "Of course, to make this concrete, we must define what it means for two or more observations to be similar or different. Indeed, this is often a domain-specific consideration that must be made based on knowledge of the data being studied.\n",
    "\n",
    "<br>\n",
    "\n",
    "<center><img src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_comparison_001.png\"></center>\n",
    "<center>clustering Algorithms in Sklearn</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means\n",
    "\n",
    "\n",
    "K-means clustering is a simple and elegant approach for partitioning a data set into K distinct, non-overlapping clusters by trying to separate samples in n groups of equal variance, minimizing a criterion known as the inertia or within-cluster sum-of-squares \n",
    "\n",
    "To perform K-means clustering, we must first specify the desired number of clusters K; then the K-means algorithm will assign each observation to exactly one of the K clusters. \n",
    "\n",
    "\n",
    "It is a general-purpose algorithm and is used when you have even cluster size, flat geometry, and not too many clusters.\n",
    "\n",
    "---\n",
    "**K-Means Clustering Algorithm**\n",
    "1. Randomly assign a number, from 1 to K, to each of the observations.\n",
    "These serve as initial cluster assignments for the observations. \n",
    "\n",
    "\n",
    "2. Iterate until the cluster assignments stop changing:\n",
    "    \n",
    "    (a) For each of the K clusters, compute the cluster centroid. The kth cluster centroid is the vector of the p feature means for the observations in the kth cluster.\n",
    "    \n",
    "    (b) Assign each observation to the cluster whose centroid is closest (where closest is defined using Euclidean distance).\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(centers=4, random_state=rng)\n",
    "\n",
    "km = KMeans(n_clusters=4, random_state=rng)\n",
    "km.fit(X)\n",
    "print(km.cluster_centers_.shape)\n",
    "print(km.labels_.shape)\n",
    "# predict is the same as labels_ on training data\n",
    "# but can be applied to new data\n",
    "print(km.predict(X).shape)\n",
    "# scatter_tab()\n",
    "ax = plt.gca()\n",
    "ax.scatter(X[:, 0], X[:, 1], c=km.labels_)\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.xticks(())\n",
    "plt.yticks(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=km.labels_)\n",
    "\n",
    "\n",
    "xlim = plt.xlim()\n",
    "ylim = plt.ylim()\n",
    "\n",
    "xs = np.linspace(xlim[0], xlim[1], 1000)\n",
    "ys = np.linspace(ylim[0], ylim[1], 1000)\n",
    "xx, yy = np.meshgrid(xs, ys)\n",
    "pred = km.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "plt.xlim(xlim)\n",
    "plt.ylim(ylim)\n",
    "plt.contourf(xx, yy, pred.reshape(xx.shape), alpha=.2, cmap='Accent')\n",
    "plt.contour(xx, yy, pred.reshape(xx.shape), colors='k')\n",
    "plt.scatter(X[:, 0], X[:, 1], s=8, alpha=.6, c=km.labels_, cmap='Accent')\n",
    "centers = km.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1],\n",
    "            s=55,\n",
    "            cmap='Accent', marker=\"^\", c=range(km.n_clusters),\n",
    "            edgecolor='k'\n",
    "           )\n",
    "\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.xticks(())\n",
    "plt.yticks(())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN\n",
    "\n",
    "\n",
    "The DBSCAN algorithm views clusters as areas of high density separated by areas of low density. Due to this rather generic view, clusters found by DBSCAN can be any shape, as opposed to k-means which assumes that clusters are convex shaped. \n",
    "\n",
    "\n",
    "The central component to the DBSCAN is the concept of core samples, which are samples that are in areas of high density. A cluster is therefore a set of core samples, each close to each other (measured by some distance measure) and a set of non-core samples that are close to a core sample (but are not themselves core samples). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your number of features is high, it may be useful to reduce it with an unsupervised step prior to supervised steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principle Component Analysis (PCA)\n",
    "\n",
    "PCA is used to decompose a multivariate dataset in a set of successive orthogonal components that explain a maximum amount of the variance.\n",
    "\n",
    "i.e. PCA is a way to bring out strong patterns from large and complex datasets. The essence of the data is captured in a few principal components, which themselves convey the most variation in the dataset. PCA reduces the number of dimensions without selecting or discarding them.\n",
    "\n",
    "<center><img src=\"https://miro.medium.com/max/499/1*V9yJUH9tVrMQI88TuIkCFQ.gif\"></center>\n",
    "\n",
    "It is often interesting to project data to a lower-dimensional space that preserves most of the variance, by dropping the singular vector of components associated with lower singular values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both clustering and PCA seek to simplify the data via a small number of summaries, but their mechanisms are different:\n",
    "\n",
    "* PCA looks to find a low-dimensional representation of the observations that explain a good fraction of the variance;\n",
    "* Clustering looks to find homogeneous subgroups among the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X.data)\n",
    "print(X_pca.shape)\n",
    "\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1])\n",
    "plt.xlabel(\"first principal component\")\n",
    "plt.ylabel(\"second principal component\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y)\n",
    "plt.xlabel(\"first principal component\")\n",
    "plt.ylabel(\"second principal component\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = pca.components_\n",
    "plt.imshow(components.T)\n",
    "plt.yticks(range(len(iris.feature_names)), iris.feature_names)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "components_array, shape (n_components, n_features)\n",
    "Principal axes in feature space, representing the directions of maximum variance in the data. The components are sorted by `explained_variance_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import scale, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y)\n",
    "plt.xlabel(\"first principal component\")\n",
    "plt.ylabel(\"second principal component\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_scaled = make_pipeline(StandardScaler(), PCA(n_components=2))\n",
    "\n",
    "\n",
    "X_pca_scaled = pca_scaled.fit_transform(X)\n",
    "plt.scatter(X_pca_scaled[:, 0], X_pca_scaled[:, 1], c=y, alpha=.9)\n",
    "plt.xlabel(\"first principal component\")\n",
    "plt.ylabel(\"second principal component\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = pca_scaled.named_steps['pca'].components_\n",
    "plt.imshow(components.T)\n",
    "plt.yticks(range(len(iris.feature_names)), iris.feature_names)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(components[0], components[1])\n",
    "for i, feature_contribution in enumerate(components.T):\n",
    "    plt.annotate(iris.feature_names[i], feature_contribution)\n",
    "plt.xlabel(\"first principal component\")\n",
    "plt.ylabel(\"second principal component\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "people = fetch_lfw_people(min_faces_per_person=20, resize=0.7)\n",
    "image_shape = people.images[0].shape\n",
    "fix, axes = plt.subplots(2, 5, figsize=(15, 8),\n",
    "subplot_kw={'xticks': (), 'yticks': ()})\n",
    "for target, image, ax in zip(people.target, people.images, axes.ravel()):\n",
    "    ax.imshow(image, cmap='gray')\n",
    "    ax.set_title(people.target_names[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.zeros(people.target.shape, dtype=np.bool)\n",
    "for target in np.unique(people.target):\n",
    "    mask[np.where(people.target == target)[0][:50]] = 1\n",
    "    \n",
    "X_people = people.data[mask]\n",
    "y_people = people.target[mask]\n",
    "# scale the grayscale values to be between 0 and 1\n",
    "# instead of 0 and 255 for better numeric stability\n",
    "X_people = X_people / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=15, whiten=True, random_state=rng).fit(X_people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix, axes = plt.subplots(3, 5, figsize=(15, 12),\n",
    "subplot_kw={'xticks': (), 'yticks': ()})\n",
    "for i, (component, ax) in enumerate(zip(pca.components_, axes.ravel())):\n",
    "    ax.imshow(component.reshape(image_shape),\n",
    "    cmap='viridis')\n",
    "    ax.set_title(\"{}. component\".format((i + 1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
