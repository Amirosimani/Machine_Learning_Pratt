{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with OpenAI GPT models\n",
    "\n",
    "just like any other APIs, you can send a request to openAI chatGPT server and get the response back from your query.\n",
    "\n",
    "[<img src=\"https://res.cloudinary.com/practicaldev/image/fetch/s--6UwyTHKO--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_800/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/t637s1yazyyxfl31ymmq.jpg\">](https://res.cloudinary.com/practicaldev/image/fetch/s--6UwyTHKO--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_800/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/t637s1yazyyxfl31ymmq.jpg)\n",
    "\n",
    "\n",
    "You need to:\n",
    "* [Set up an account](https://auth0.openai.com/u/signup/identifier?state=hKFo2SBLZVEyMlJSRDNkbWVMUWVYdU5SVGZKQWltY016ek1POaFur3VuaXZlcnNhbC1sb2dpbqN0aWTZIEJxeTRsb191RnZySEV0b2dlYnRZdGNzQWpZdkRWZjI4o2NpZNkgRFJpdnNubTJNdTQyVDNLT3BxZHR3QjNOWXZpSFl6d0Q)\n",
    "* [Get an API Key](https://platform.openai.com/api-keys)\n",
    "* Add money!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from datasets import load_dataset\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# I created a local config.py file to manage my secret keys\n",
    "from config import API_KEY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text classification with a LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and cache the dataset:\n",
    "raw_datasets = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets['train']['text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets['train']['label'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add info on temp and other params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params ={\n",
    "    \"model_name\": \"gpt-3.5-turbo\",\n",
    "    \"temperature\": 0.1,\n",
    "    \"max_tokens\":256\n",
    "}\n",
    "\n",
    "def classifier(input_text, parameters, client=client):\n",
    "    \n",
    "    \n",
    "    messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a useful assitant for the imdb website. You should read the submitted movie review by a user below and decide if it is a positive or negative. return the result with 0 or 1 for negative and positive respectively\"},\n",
    "    {\"role\": \"user\", \"content\": input_text}\n",
    "    ]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=parameters[\"model_name\"],\n",
    "        messages=messages,\n",
    "        temperature=parameters[\"temperature\"], \n",
    "        max_tokens=parameters[\"max_tokens\"],\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier(raw_datasets['train']['text'][1], params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can now run the same query on all the rows and get the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select random 20 review and their label\n",
    "random_idx = random.sample(range(1, 25000), 20)\n",
    "\n",
    "\n",
    "sel_text = [raw_datasets['train']['text'][i] for i in random_idx]\n",
    "sel_labels = [raw_datasets['train']['label'][i] for i in random_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn to a dataframe to make it easier to see and manipulate data/\n",
    "df = pd.DataFrame([sel_text, sel_labels]).T\n",
    "df.columns = ['text', 'label']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pro tip**: **Partial Functions**\n",
    "\n",
    "A partial function allows us to call a second function with fixed values in certain arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "classifier_pd = partial(classifier, parameters=params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df['predicted'] = df['text'].apply(classifier_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can't get away from data cleaning!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted'] = df['predicted'].apply(lambda x: '1' if \"positive\" in x else x)\n",
    "df['predicted'] = df['predicted'].apply(lambda x: '0' if \"negative\" in x else x)\n",
    "\n",
    "df[[\"label\", \"predicted\"]] = df[[\"label\", \"predicted\"]].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(df[\"label\"], df[\"predicted\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain\n",
    "\n",
    "LangChain provides many modules that can be used to build language model applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain\n",
    "!pip install typing-inspect==0.8.0\n",
    "!pip install typing_extensions==4.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from config import API_KEY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Model: The language model is the core reasoning engine.\n",
    "llm = ChatOpenAI(openai_api_key=API_KEY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two types of language models:\n",
    "\n",
    "* `LLM`: underlying model takes a string as input and returns a string\n",
    "* `ChatModel`: underlying model takes a list of messages as input and returns a message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = raw_datasets['train']['text'][1]\n",
    "\n",
    "text = f\"\"\"\n",
    "You are a useful assitant for the imdb website.\n",
    "You should read the submitted movie review by a user below and decide if it is a positive or negative.\n",
    "Return the result with 0 or 1 for negative and positive respectively.\n",
    "\n",
    "{review}\n",
    "\"\"\"\n",
    "\n",
    "messages = [HumanMessage(content=text)]\n",
    "messages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to call an LLM or ChatModel is using `.invoke()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prompt template\n",
    "\n",
    "When you don't want to pass user input directly into an LLM, you can add the it to a larger piece of text, called a `prompt template`. It provides additional context on the specific task at hand.\n",
    "\n",
    "In the previous example, the text we passed to the model contained instructions. By using prompt templates, we only have to provide the review itself, without having to worry about giving the model instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a useful assitant for the imdb website.\n",
    "You should read the submitted movie review by a user below and decide if it is a positive or negative.\n",
    "Return the result with 0 or 1 for negative and positive respectively.\n",
    "\n",
    "{review}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "prompt.format(review=raw_datasets['train']['text'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output parser\n",
    "\n",
    "`OutputParsers` convert the raw output of a language model into a format that can be used downstream.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "class BooleanParser(BaseOutputParser):\n",
    "    \"\"\"Parse the output of an LLM call to 0 and 1.\"\"\"\n",
    "\n",
    "\n",
    "    def parse(self, text: str):\n",
    "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
    "        r = 0 if 'negative' in s else 1\n",
    "        return r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "We can now combine all these into one chain. This chain will take input variables, pass those to a prompt template to create a prompt, pass the prompt to a language model, and then pass the output through an (optional) output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BooleanParser(BaseOutputParser):\n",
    "    \"\"\"Parse the output of an LLM call to 0 and 1.\"\"\"\n",
    "\n",
    "\n",
    "    def parse(self, text: str):\n",
    "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
    "        r = 0 if 'negative' in text else 1\n",
    "        return r\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a useful assitant for the imdb website.\n",
    "You should read the submitted movie review by a user below and decide if it is a positive or negative.\n",
    "Return the result with 0 or 1 for negative and positive respectively.\n",
    "\n",
    "{review}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | BooleanParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"review\": raw_datasets['train']['text'][1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Code Assitant](https://python.langchain.com/docs/expression_language/cookbook/code_writing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain_experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "\n",
    "from config import API_KEY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Write some python code to solve the user's problem. \n",
    "\n",
    "Return only python code in Markdown format, e.g.:\n",
    "\n",
    "```python\n",
    "....\n",
    "```\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", template), (\"human\", \"{input}\")])\n",
    "\n",
    "model = ChatOpenAI(api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sanitize_output(text: str):\n",
    "    _, after = text.split(\"```python\")\n",
    "    return after.split(\"```\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model | StrOutputParser() | _sanitize_output | PythonREPL().run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ModuleNotFoundError(\"No module named \\'xgboost\\'\")'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"use sklearn pipelines to create a xgboost classifier pipeline with scaling \"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
