{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative AI (GenAI)\n",
    "\n",
    "\n",
    "Generative AI (GenAI) is a category of artificial intelligence that focuses on creating new content.  GenAI systems utilize deep learning models trained on large datasets to generate various forms of media, such as text, images, and audio. Unlike traditional AI systems that typically analyze existing data, GenAI employs generative models to produce new data instances with similar characteristics to the training data. This has applications in areas like automating creative tasks and aiding in research.\n",
    "\n",
    "To use Gen AI models, you need:\n",
    "\n",
    "* **Basic Programming**: Familiarity with Python is essential for most GenAI tasks, as many popular libraries and frameworks are Python-based.\n",
    "* **Data Literacy**: Understanding data formats, cleaning and preprocessing data, and basic data analysis skills are important for working with the datasets used to train and evaluate GenAI models.\n",
    "* **Machine Learning Fundamentals**: A basic grasp of machine learning concepts like training, validation, and overfitting can help you understand how GenAI models work and how to optimize their performance.\n",
    "* **Prompt Engineering**: This involves crafting effective prompts or inputs to guide the GenAI model towards generating the desired output. It requires a combination of clear communication, domain knowledge, and an understanding of the model's capabilities and limitations.\n",
    "* **Model Selection and Fine-tuning**: Knowing how to choose the right GenAI model for a specific task and fine-tuning existing models for optimal results is becoming increasingly important.\n",
    "* **API Usage**: Many GenAI models are accessed through APIs. Learning how to interact with these APIs is crucial for integrating GenAI into applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is an LLM?\n",
    "\n",
    "![Prompt structure](./img/llm.png)\n",
    "\n",
    "Large Language Models are a type of machine learning algorithm that can understand, generate, and manipulate human language. They are trained on massive datasets of text and code, enabling them to perform a wide range of tasks.\n",
    "\n",
    "**Key Characteristics:**\n",
    "\n",
    "* **Foundation Models:** LLMs are often considered foundation models, meaning they can be adapted to various downstream tasks through fine-tuning.\n",
    "* **Data Scale:**  Trained on petabytes of text data, resulting in models with billions of parameters.\n",
    "* **Generative Capabilities:** LLMs can generate human-quality text, translate languages, write different kinds of creative content, and answer your questions in an informative way.\n",
    "\n",
    "**Training Process:**\n",
    "\n",
    "1.  **Pre-training:** LLMs are initially trained on a massive, general-purpose text dataset. This allows them to learn the underlying structure and patterns of language.\n",
    "2.  **Fine-tuning:**  The pre-trained model is then fine-tuned on a smaller, task-specific dataset to adapt it for a particular application (e.g., translation, question answering, code generation).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLM Parameters\n",
    "\n",
    "- **Number of tokens**:  allows you to set a limit to how many tokens are generated.\n",
    "- **Temperature**: controls the randomness of the LLM's output. the lower the temperature, the more deterministic the response is.\n",
    "- **Top-k**:  tells the model to pick the next token from the top ‘k’ tokens in its list, sorted by probability.\n",
    "- **Top-p**: is similar to `top-k`` but picks from the top tokens based on the sum of their probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt Engineering\n",
    "\n",
    "Prompt art the text you feed to your model. Prompt engineering is like guiding a conversation with an AI. It is also called prompting, prompt deisng, in-context learning.\n",
    "\n",
    "#### Types of Prompt Engineering\n",
    "\n",
    "\n",
    "1. **Zero-Shot Prompting**\n",
    "Zero-shot prompting asks the model to perform a task without providing examples. \n",
    "\n",
    "**Prompt:**  \n",
    "\"Summarize the following text: 'Artificial Intelligence is revolutionizing industries by automating tasks and providing insights.'\"\n",
    "\n",
    "**Response:**  \n",
    "\"AI is transforming industries by automating tasks and offering insights.\"\n",
    "\n",
    "---\n",
    "\n",
    "2. **One-Shot Prompting**\n",
    "In one-shot prompting, you provide a single example to guide the model.\n",
    "\n",
    "**Prompt:**  \n",
    "\"Translate this text to French: 'Hello, how are you?' Example: English: 'Good morning' → French: 'Bonjour' English: 'Hello, how are you?' →\"\n",
    "\n",
    "**Response:**  \n",
    "\"Bonjour, comment ça va?\"\n",
    "\n",
    "---\n",
    "\n",
    "3. **Few-Shot Prompting**\n",
    "Few-shot prompting includes several examples to define the desired behavior explicitly.\n",
    "\n",
    "**Prompt:**  \n",
    "\"Classify the sentiment of these sentences:  \n",
    "1. 'I love this product!' → Positive  \n",
    "2. 'This is the worst experience ever.' → Negative  \n",
    "3. 'It’s okay, not great.' →\"\n",
    "\n",
    "**Response:**  \n",
    "\"Neutral.\"\n",
    "\n",
    "---\n",
    "\n",
    "4. **Chain-of-Thought (CoT) Prompting**\n",
    "CoT prompting encourages the model to reason step by step.\n",
    "\n",
    "**Prompt:**  \n",
    "\"A farmer has 10 apples. He gives 3 to his neighbor and buys 5 more. How many apples does he have now? Explain step-by-step.\"\n",
    "\n",
    "**Response:**  \n",
    "\"The farmer starts with 10 apples. He gives away 3, leaving him with 7. Then, he buys 5 more, so he has 7 + 5 = 12 apples.\"\n",
    "\n",
    "---\n",
    "\n",
    "5. **Instruction-based Prompting**\n",
    "Explicit instructions are used to guide the model.\n",
    "\n",
    "**Prompt:**  \n",
    "\"Write a polite email to request a meeting with your manager to discuss project updates.\"\n",
    "\n",
    "**Response:**  \n",
    "\"Subject: Request for Meeting to Discuss Project Updates  \n",
    "Dear [Manager's Name],  \n",
    "I hope this email finds you well. I would like to request a meeting at your earliest convenience to provide updates on our project and discuss the next steps. Please let me know your availability.  \n",
    "Best regards,  \n",
    "[Your Name]\"\n",
    "\n",
    "---\n",
    "\n",
    "6. **Few-Shot Chain-of-Thought (CoT) Prompting**\n",
    "Combines few-shot examples with step-by-step reasoning.\n",
    "\n",
    "**Prompt:**  \n",
    "\"Solve these problems step-by-step:  \n",
    "1. A person has $20. They spend $7 on a meal and $5 on a drink. How much is left? → $20 - $7 = $13, $13 - $5 = $8  \n",
    "2. A student scores 85 in math, 90 in science, and 80 in English. What is the average score? →\"\n",
    "\n",
    "**Response:**  \n",
    "\"(85 + 90 + 80) / 3 = 255 / 3 = 85.\"\n",
    "\n",
    "\n",
    "#### Elements of a Prompt\n",
    "\n",
    "![Prompt structure](./img/costar.png)\n",
    "\n",
    "\n",
    "\n",
    "CO-STAR stands for:\n",
    "(C) Context: Provide background information on the task.\n",
    "(O) Objective: Define what the task is that you want the LLM to perform.\n",
    "(S) Style: Specify the writing style you want the LLM to use.\n",
    "(T) Tone: Set the attitude of the response.\n",
    "(A) Audience: Identify who the response is intended for.\n",
    "(R) Response: Provide the response format.\n",
    "\n",
    "\n",
    "#### Effective Rrompt Engineering\n",
    "\n",
    "Here's how to make it more effective:\n",
    "\n",
    "**1. Guide, Don't Just Ask:**\n",
    "\n",
    "*   **Think beyond keywords:** Provide context, constraints, and examples.\n",
    "*   **Be specific:** Clear instructions lead to better understanding.\n",
    "*   **Iterate and refine:** Experiment and analyze results to improve your approach.\n",
    "\n",
    "**2. Understand the AI:**\n",
    "\n",
    "*   **Know its capabilities:** Each AI has strengths in different areas (stories, code, etc.).\n",
    "*   **Be aware of limitations:** AI can sometimes generate incorrect or biased output.\n",
    "\n",
    "**3. Craft Effective Prompts:**\n",
    "\n",
    "*   **Use clear language:** Avoid ambiguity and jargon.\n",
    "*   **Provide context:** Set the scene, define the tone, or give background information.\n",
    "*   **Structure your prompts:** Use formatting, separators, and keywords.\n",
    "*   **Give examples:** Show the AI what you're looking for.\n",
    "*   **Experiment:** Try different prompt styles (role-playing, question-answering, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with GenAI models\n",
    "\n",
    "Many GenAI models are considered MaaS (Model as a Service). just like any other APIs, you can send a request to their server and get the response back from your query. For example, for GPT models:\n",
    "\n",
    "[<img src=\"https://res.cloudinary.com/practicaldev/image/fetch/s--6UwyTHKO--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_800/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/t637s1yazyyxfl31ymmq.jpg\">](https://res.cloudinary.com/practicaldev/image/fetch/s--6UwyTHKO--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_800/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/t637s1yazyyxfl31ymmq.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Gemini\n",
    "\n",
    "* [Get an API key](https://ai.google.dev/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT models\n",
    "\n",
    "\n",
    "You need to:\n",
    "* [Set up an account](https://auth0.openai.com/u/signup/identifier?state=hKFo2SBLZVEyMlJSRDNkbWVMUWVYdU5SVGZKQWltY016ek1POaFur3VuaXZlcnNhbC1sb2dpbqN0aWTZIEJxeTRsb191RnZySEV0b2dlYnRZdGNzQWpZdkRWZjI4o2NpZNkgRFJpdnNubTJNdTQyVDNLT3BxZHR3QjNOWXZpSFl6d0Q)\n",
    "* [Get an API Key](https://platform.openai.com/api-keys)\n",
    "* Add money!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from datasets import load_dataset\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# I created a local config.py file to manage my secret keys\n",
    "from config import API_KEY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text classification with a LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and cache the dataset:\n",
    "raw_datasets = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(raw_datasets['train']['text'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets['train']['label'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "params ={\n",
    "    \"model_name\": \"gpt-3.5-turbo\",\n",
    "    \"temperature\": 0.1,\n",
    "    \"max_tokens\":256\n",
    "}\n",
    "\n",
    "def classifier(input_text, parameters, client=client):\n",
    "    \n",
    "    \n",
    "    messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a useful assistant for the IMDb website. Your task is to read the submitted movie review below and determine whether it is positive or negative. Please return the result as 0 for negative and 1 for positive.\"},\n",
    "    {\"role\": \"user\", \"content\": input_text}\n",
    "    ]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=parameters[\"model_name\"],\n",
    "        messages=messages,\n",
    "        temperature=parameters[\"temperature\"], \n",
    "        max_tokens=parameters[\"max_tokens\"],\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier(raw_datasets['train']['text'][1], params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can now run the same query on all the rows and get the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select random 20 review and their label\n",
    "random_idx = random.sample(range(1, 25000), 4)\n",
    "\n",
    "\n",
    "sel_text = [raw_datasets['train']['text'][i] for i in random_idx]\n",
    "sel_labels = [raw_datasets['train']['label'][i] for i in random_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn to a dataframe to make it easier to see and manipulate data/\n",
    "df = pd.DataFrame([sel_text, sel_labels]).T\n",
    "df.columns = ['text', 'label']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pro tip**: **Partial Functions**\n",
    "\n",
    "A partial function allows us to call a second function with fixed values in certain arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "classifier_pd = partial(classifier, parameters=params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df['predicted'] = df['text'].apply(classifier_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can't get away from data cleaning!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted'] = df['predicted'].apply(lambda x: '1' if \"positive\" in x else x)\n",
    "df['predicted'] = df['predicted'].apply(lambda x: '0' if \"negative\" in x else x)\n",
    "\n",
    "df[[\"label\", \"predicted\"]] = df[[\"label\", \"predicted\"]].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(df[\"label\"], df[\"predicted\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain\n",
    "\n",
    "LangChain provides many modules that can be used to build language model applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain\n",
    "!pip install typing-inspect==0.8.0\n",
    "!pip install typing_extensions==4.5.0\n",
    "!pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from config import API_KEY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Model: The language model is the core reasoning engine.\n",
    "llm = ChatOpenAI(openai_api_key=API_KEY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two types of language models:\n",
    "\n",
    "* `LLM`: underlying model takes a string as input and returns a string\n",
    "* `ChatModel`: underlying model takes a list of messages as input and returns a message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = raw_datasets['train']['text'][1]\n",
    "\n",
    "text = f\"\"\"\n",
    "You are a useful assitant for the imdb website.\n",
    "You should read the submitted movie review by a user below and decide if it is a positive or negative.\n",
    "Return the result with 0 or 1 for negative and positive respectively.\n",
    "\n",
    "{review}\n",
    "\"\"\"\n",
    "\n",
    "messages = [HumanMessage(content=text)]\n",
    "messages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to call an LLM or ChatModel is using `.invoke()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prompt template\n",
    "\n",
    "When you don't want to pass user input directly into an LLM, you can add the it to a larger piece of text, called a `prompt template`. It provides additional context on the specific task at hand.\n",
    "\n",
    "In the previous example, the text we passed to the model contained instructions. By using prompt templates, we only have to provide the review itself, without having to worry about giving the model instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a useful assitant for the imdb website.\n",
    "You should read the submitted movie review by a user below and decide if it is a positive or negative.\n",
    "Return the result with 0 or 1 for negative and positive respectively.\n",
    "\n",
    "{review}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "prompt.format(review=raw_datasets['train']['text'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output parser\n",
    "\n",
    "`OutputParsers` convert the raw output of a language model into a format that can be used downstream.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "class BooleanParser(BaseOutputParser):\n",
    "    \"\"\"Parse the output of an LLM call to 0 and 1.\"\"\"\n",
    "\n",
    "\n",
    "    def parse(self, text: str):\n",
    "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
    "        r = 0 if 'negative' in text else 1\n",
    "        return r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "We can now combine all these into one chain. This chain will take input variables, pass those to a prompt template to create a prompt, pass the prompt to a language model, and then pass the output through an (optional) output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BooleanParser(BaseOutputParser):\n",
    "    \"\"\"Parse the output of an LLM call to 0 and 1.\"\"\"\n",
    "\n",
    "\n",
    "    def parse(self, text: str):\n",
    "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
    "        r = 0 if 'negative' in text else 1\n",
    "        return r\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a useful assitant for the imdb website.\n",
    "You should read the submitted movie review by a user below and decide if it is a positive or negative.\n",
    "Return the result with 0 or 1 for negative and positive respectively.\n",
    "\n",
    "{review}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | BooleanParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"review\": raw_datasets['train']['text'][1]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
