{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative AI (GenAI)\n",
    "\n",
    "\n",
    "Generative AI (GenAI) is a category of artificial intelligence that focuses on creating new content.  GenAI systems utilize deep learning models trained on large datasets to generate various forms of media, such as text, images, and audio. Unlike traditional AI systems that typically analyze existing data, GenAI employs generative models to produce new data instances with similar characteristics to the training data. This has applications in areas like automating creative tasks and aiding in research.\n",
    "\n",
    "To use Gen AI models, you need:\n",
    "\n",
    "* **Basic Programming**: Familiarity with Python is essential for most GenAI tasks, as many popular libraries and frameworks are Python-based.\n",
    "* **Data Literacy**: Understanding data formats, cleaning and preprocessing data, and basic data analysis skills are important for working with the datasets used to train and evaluate GenAI models.\n",
    "* **Machine Learning Fundamentals**: A basic grasp of machine learning concepts like training, validation, and overfitting can help you understand how GenAI models work and how to optimize their performance.\n",
    "* **Prompt Engineering**: This involves crafting effective prompts or inputs to guide the GenAI model towards generating the desired output. It requires a combination of clear communication, domain knowledge, and an understanding of the model's capabilities and limitations.\n",
    "* **Model Selection and Fine-tuning**: Knowing how to choose the right GenAI model for a specific task and fine-tuning existing models for optimal results is becoming increasingly important.\n",
    "* **API Usage**: Many GenAI models are accessed through APIs. Learning how to interact with these APIs is crucial for integrating GenAI into applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is an LLM?\n",
    "\n",
    "![Prompt structure](./img/llm.png)\n",
    "\n",
    "Large Language Models are a type of machine learning algorithm that can understand, generate, and manipulate human language. They are trained on massive datasets of text and code, enabling them to perform a wide range of tasks.\n",
    "\n",
    "**Key Characteristics:**\n",
    "\n",
    "* **Foundation Models:** LLMs are often considered foundation models, meaning they can be adapted to various downstream tasks through fine-tuning.\n",
    "* **Data Scale:**  Trained on petabytes of text data, resulting in models with billions of parameters.\n",
    "* **Generative Capabilities:** LLMs can generate human-quality text, translate languages, write different kinds of creative content, and answer your questions in an informative way.\n",
    "\n",
    "**Training Process:**\n",
    "\n",
    "1.  **Pre-training:** LLMs are initially trained on a massive, general-purpose text dataset. This allows them to learn the underlying structure and patterns of language.\n",
    "2.  **Fine-tuning:**  The pre-trained model is then fine-tuned on a smaller, task-specific dataset to adapt it for a particular application (e.g., translation, question answering, code generation).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLM Parameters\n",
    "\n",
    "- **Number of tokens**:  allows you to set a limit to how many tokens are generated.\n",
    "- **Temperature**: controls the randomness of the LLM's output. the lower the temperature, the more deterministic the response is.\n",
    "- **Top-k**:  tells the model to pick the next token from the top ‘k’ tokens in its list, sorted by probability.\n",
    "- **Top-p**: is similar to `top-k`` but picks from the top tokens based on the sum of their probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt Engineering\n",
    "\n",
    "Prompt art the text you feed to your model. Prompt engineering is like guiding a conversation with an AI. It is also called prompting, prompt deisng, in-context learning.\n",
    "\n",
    "#### Types of Prompt Engineering\n",
    "\n",
    "\n",
    "1. **Zero-Shot Prompting**\n",
    "Zero-shot prompting asks the model to perform a task without providing examples. \n",
    "\n",
    "**Prompt:**  \n",
    "\"Summarize the following text: 'Artificial Intelligence is revolutionizing industries by automating tasks and providing insights.'\"\n",
    "\n",
    "**Response:**  \n",
    "\"AI is transforming industries by automating tasks and offering insights.\"\n",
    "\n",
    "---\n",
    "\n",
    "2. **One-Shot Prompting**\n",
    "In one-shot prompting, you provide a single example to guide the model.\n",
    "\n",
    "**Prompt:**  \n",
    "\"Translate this text to French: 'Hello, how are you?' Example: English: 'Good morning' → French: 'Bonjour' English: 'Hello, how are you?' →\"\n",
    "\n",
    "**Response:**  \n",
    "\"Bonjour, comment ça va?\"\n",
    "\n",
    "---\n",
    "\n",
    "3. **Few-Shot Prompting**\n",
    "Few-shot prompting includes several examples to define the desired behavior explicitly.\n",
    "\n",
    "**Prompt:**  \n",
    "\"Classify the sentiment of these sentences:  \n",
    "1. 'I love this product!' → Positive  \n",
    "2. 'This is the worst experience ever.' → Negative  \n",
    "3. 'It’s okay, not great.' →\"\n",
    "\n",
    "**Response:**  \n",
    "\"Neutral.\"\n",
    "\n",
    "---\n",
    "\n",
    "4. **Chain-of-Thought (CoT) Prompting**\n",
    "CoT prompting encourages the model to reason step by step.\n",
    "\n",
    "**Prompt:**  \n",
    "\"A farmer has 10 apples. He gives 3 to his neighbor and buys 5 more. How many apples does he have now? Explain step-by-step.\"\n",
    "\n",
    "**Response:**  \n",
    "\"The farmer starts with 10 apples. He gives away 3, leaving him with 7. Then, he buys 5 more, so he has 7 + 5 = 12 apples.\"\n",
    "\n",
    "---\n",
    "\n",
    "5. **Instruction-based Prompting**\n",
    "Explicit instructions are used to guide the model.\n",
    "\n",
    "**Prompt:**  \n",
    "\"Write a polite email to request a meeting with your manager to discuss project updates.\"\n",
    "\n",
    "**Response:**  \n",
    "\"Subject: Request for Meeting to Discuss Project Updates  \n",
    "Dear [Manager's Name],  \n",
    "I hope this email finds you well. I would like to request a meeting at your earliest convenience to provide updates on our project and discuss the next steps. Please let me know your availability.  \n",
    "Best regards,  \n",
    "[Your Name]\"\n",
    "\n",
    "---\n",
    "\n",
    "6. **Few-Shot Chain-of-Thought (CoT) Prompting**\n",
    "Combines few-shot examples with step-by-step reasoning.\n",
    "\n",
    "**Prompt:**  \n",
    "\"Solve these problems step-by-step:  \n",
    "1. A person has $20. They spend $7 on a meal and $5 on a drink. How much is left? → $20 - $7 = $13, $13 - $5 = $8  \n",
    "2. A student scores 85 in math, 90 in science, and 80 in English. What is the average score? →\"\n",
    "\n",
    "**Response:**  \n",
    "\"(85 + 90 + 80) / 3 = 255 / 3 = 85.\"\n",
    "\n",
    "\n",
    "#### Elements of a Prompt\n",
    "\n",
    "![Prompt structure](./img/costar.png)\n",
    "\n",
    "\n",
    "\n",
    "CO-STAR stands for:\n",
    "(C) Context: Provide background information on the task.\n",
    "(O) Objective: Define what the task is that you want the LLM to perform.\n",
    "(S) Style: Specify the writing style you want the LLM to use.\n",
    "(T) Tone: Set the attitude of the response.\n",
    "(A) Audience: Identify who the response is intended for.\n",
    "(R) Response: Provide the response format.\n",
    "\n",
    "\n",
    "#### Effective Rrompt Engineering\n",
    "\n",
    "Here's how to make it more effective:\n",
    "\n",
    "**1. Guide, Don't Just Ask:**\n",
    "\n",
    "*   **Think beyond keywords:** Provide context, constraints, and examples.\n",
    "*   **Be specific:** Clear instructions lead to better understanding.\n",
    "*   **Iterate and refine:** Experiment and analyze results to improve your approach.\n",
    "\n",
    "**2. Understand the AI:**\n",
    "\n",
    "*   **Know its capabilities:** Each AI has strengths in different areas (stories, code, etc.).\n",
    "*   **Be aware of limitations:** AI can sometimes generate incorrect or biased output.\n",
    "\n",
    "**3. Craft Effective Prompts:**\n",
    "\n",
    "*   **Use clear language:** Avoid ambiguity and jargon.\n",
    "*   **Provide context:** Set the scene, define the tone, or give background information.\n",
    "*   **Structure your prompts:** Use formatting, separators, and keywords.\n",
    "*   **Give examples:** Show the AI what you're looking for.\n",
    "*   **Experiment:** Try different prompt styles (role-playing, question-answering, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with GenAI models\n",
    "\n",
    "Many GenAI models are considered MaaS (Model as a Service). just like any other APIs, you can send a request to their server and get the response back from your query. For example, for GPT models:\n",
    "\n",
    "[<img src=\"https://res.cloudinary.com/practicaldev/image/fetch/s--6UwyTHKO--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_800/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/t637s1yazyyxfl31ymmq.jpg\">](https://res.cloudinary.com/practicaldev/image/fetch/s--6UwyTHKO--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_800/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/t637s1yazyyxfl31ymmq.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Gemini\n",
    "\n",
    "* [Get an API key](https://ai.google.dev/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from datasets import load_dataset\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "# I created a local config.py file to manage my secret keys\n",
    "from config import GEMINI_API_KEY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and cache IMDB dataset:\n",
    "raw_datasets = load_dataset(\"imdb\")\n",
    "pprint(raw_datasets['train']['text'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "# Create the model\n",
    "generation_config = {\n",
    "  \"temperature\": 2,\n",
    "  \"top_p\": 0.95,\n",
    "  \"top_k\": 40,\n",
    "  \"max_output_tokens\": 8192,\n",
    "  \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "  model_name=\"gemini-1.5-pro\",\n",
    "  generation_config=generation_config,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(text):\n",
    "    chat_session = model.start_chat(\n",
    "        history=[\n",
    "            {\"role\": \"user\", \"parts\": [\"Analyze the sentiment of the following movie review and classify it as POSITIVE, NEGATIVE, or NEUTRAL. \\n\\n\\\"I saw this movie last night and it was absolutely incredible! The acting was superb, the plot was gripping, and the special effects were mind-blowing. I highly recommend it to anyone who enjoys action-packed sci-fi films.\\\"\"]},\n",
    "            {\"role\": \"model\", \"parts\": [\"POSITIVE\"]},\n",
    "            {\"role\": \"user\", \"parts\": [\"Analyze the sentiment of the following movie review and classify it as POSITIVE, NEGATIVE, or NEUTRAL. \\n\\n\\\"This movie was a complete waste of time. The plot was nonsensical, the acting was wooden, and the special effects were laughably bad. I can't believe I wasted two hours of my life on this garbage.\\\"\"]},\n",
    "            {\"role\": \"model\", \"parts\": [\"NEGATIVE\"]},\n",
    "        ]\n",
    "    )\n",
    "    response = chat_session.send_message(text)\n",
    "    return response.text\n",
    "\n",
    "# Example usage with the first 5 reviews from the training set\n",
    "for i in range(5):\n",
    "    review = raw_datasets['train']['text'][i]\n",
    "    sentiment = analyze_sentiment(review)\n",
    "    print(f\"Review {i+1}: {sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT models\n",
    "\n",
    "\n",
    "You need to:\n",
    "* [Set up an account](https://auth0.openai.com/u/signup/identifier?state=hKFo2SBLZVEyMlJSRDNkbWVMUWVYdU5SVGZKQWltY016ek1POaFur3VuaXZlcnNhbC1sb2dpbqN0aWTZIEJxeTRsb191RnZySEV0b2dlYnRZdGNzQWpZdkRWZjI4o2NpZNkgRFJpdnNubTJNdTQyVDNLT3BxZHR3QjNOWXZpSFl6d0Q)\n",
    "* [Get an API Key](https://platform.openai.com/api-keys)\n",
    "* Add money!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# I created a local config.py file to manage my secret keys\n",
    "from config import API_KEY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "model=\"gpt-3.5-turbo\",\n",
    "messages=[\n",
    "    {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You will be provided with a tweet, and your task is to classify its sentiment as positive, neutral, or negative.\"\n",
    "    },\n",
    "    {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": f\"{raw_datasets['train']['text'][i]}\"\n",
    "    }\n",
    "],\n",
    "temperature=0.7,\n",
    "max_tokens=64,\n",
    "top_p=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
