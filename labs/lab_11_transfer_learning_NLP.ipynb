{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd21279-0580-4a0b-9b03-4238dcc53533",
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda install -c conda-forge ipywidgets\n",
    "# %jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58a7801-ff47-4b79-a484-cc70f93b5618",
   "metadata": {},
   "source": [
    "# What is Transfer Learning?\n",
    "\n",
    "In recent years, models have become increasingly larger (billions of parameters) and more and more labeled data has become avaialble. With more powerful infrastructure for holding that data and trainign the models (mainly thanks to the cloud), we have become increasingly good at training deep neural networks to learn a very accurate mapping from inputs to outputs.\n",
    "\n",
    "\n",
    "However, those models are usually trained on a specific dataset and for a specific task. In real world, you deal with messy data and new scenarios, many of which your model has not encountered during training and for which it is in turn ill-prepared to make predictions.\n",
    "\n",
    "The ability to transfer knowledge to new conditions is generally known as **transfer learning**.\n",
    "\n",
    "----\n",
    "\n",
    "Figure 1 shows a classic supervised learning scenario. You train a model for some task and domain A, assuming that labeled data for the same task and domain is provided.  On another occasion, when given data for some other task or domain B, we require again labeled data of the same task or domain that we can use to train a new model B so that we can expect it to perform well on this data.\n",
    "\n",
    "<img src=\"./img/lab_12_ml.png\">\n",
    "Figure 1: [The traditional supervised learning setup in ML](https://ruder.io/transfer-learning/)\n",
    "\n",
    "In the other hand, Transfer learning allows us to deal with these scenarios by leveraging the already existing labeled data of some related task or domain. We try to store this knowledge gained in solving the source task in the source domain and apply it to our problem of interest as can be seen in Figure 2.\n",
    "\n",
    "\n",
    "<img src=\"./img/lab_12_tf.png\">\n",
    "\n",
    "Figure 2: [Transfer learning setup](https://ruder.io/transfer-learning/)\n",
    "\n",
    "------\n",
    "\n",
    "### Steps of Transfer Learning\n",
    "\n",
    "Transfer learning consists of taking features learned on one problem, and leveraging them on a new, similar problem. Transfer learning is usually done for tasks where your dataset has too little data to train a full-scale model from scratch.\n",
    "\n",
    "For instance, features from a model that has learned to identify racoons may be useful to kick-start a model meant to identify tanukis.\n",
    "\n",
    "A common transfer learning workflow consists of:\n",
    "\n",
    "* Take layers from a previously trained model.\n",
    "* Freeze them, so as to avoid destroying any of the information they contain during future training rounds.\n",
    "* Add some new, trainable layers on top of the frozen layers. They will learn to turn the old features into predictions on a new dataset.\n",
    "* Train the new layers on your dataset.\n",
    "\n",
    "A last, optional step, is fine-tuning, which consists of unfreezing the entire model you obtained above (or part of it), and re-training it on the new data with a very low learning rate. This can potentially achieve meaningful improvements, by incrementally adapting the pretrained features to the new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609926be-e827-40e1-9241-2aac57472f89",
   "metadata": {},
   "source": [
    "## What is Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259a701b-6622-4b68-822b-7f2c42f43d68",
   "metadata": {},
   "source": [
    "# What is BERT?\n",
    "\n",
    "<img src=\"./img/lab_12_bert.jpg\">\n",
    "\n",
    "**BERT** stands for Bidirectional Encoder Representations from Transformers. Jacob Devlin and his colleagues developed BERT at Google in 2018. Devlin and his colleagues trained the BERT on English Wikipedia (2,500M words) and BooksCorpus (800M words) and achieved the best accuracies for some of the NLP tasks in 2018. \n",
    "\n",
    "\n",
    "There are two pre-trained general BERT variations: The base model is a 12-layer, 768-hidden, 12-heads, 110M parameter neural network architecture, whereas the large model is a 24-layer, 1024-hidden, 16-heads, 340M parameter neural network architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ac28b4-46e7-4607-84b6-957730cceb13",
   "metadata": {},
   "source": [
    "# What is Hugging Face ðŸ¤—\n",
    "\n",
    "Hugging Face is an open-source provider of natural language processing (NLP) technologies. It has a large open-source community, in particular around the Transformers library.\n",
    "\n",
    "ðŸ¤—/Transformers is a python-based library that exposes an API to use many well-known transformer architectures, such as BERT, RoBERTa, GPT-2 or DistilBERT, that obtain state-of-the-art results on a variety of NLP tasks like text classification, information extraction, question answering, and text generation. Those architectures come pre-trained with several sets of weights. Getting started with Transformers only requires to install the pip package:\n",
    "\n",
    "`pip install transformers`\n",
    "\n",
    "\n",
    "more here: https://blog.tensorflow.org/2019/11/hugging-face-state-of-art-natural.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f607a7f2-c212-4373-afe2-40ee3d297c80",
   "metadata": {},
   "source": [
    "The advantage of using `Transformers` lies in the straight-forward model-agnostic API. Loading a pre-trained model, along with its tokenizer can be done in a few lines of code. Here is an example of loading the BERT TensorFlow models as well as their tokenizers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02920a72-2843-4270-ada2-8c744dc6c892",
   "metadata": {},
   "source": [
    "# Fine-tuning a pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031533b3-afce-4f95-a878-1e73ff845701",
   "metadata": {},
   "source": [
    "We are gping to use the IMDB dataset: the task is to classify whether movie reviews are positive or negative. For more infromation you can check Datasets [documentation](https://huggingface.co/docs/datasets/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49067b9a-1870-4f4a-a92e-fc03957e6982",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/gt/m3tbcpvj4k33rzs972f8vb600000gn/T/ipykernel_62592/306833097.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# download and cache the dataset:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mraw_datasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"imdb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# download and cache the dataset:\n",
    "raw_datasets = load_dataset(\"imdb\")\n",
    "\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f013160b-3d75-46e7-b448-efd55cfb866a",
   "metadata": {},
   "source": [
    "To preprocess our data, we will need a tokenizer. If you plan on using a pretrained model, itâ€™s important to use the associated pretrained tokenizer: it will split the text you give it in tokens the same way for the pretraining corpus, and it will use the same correspondence token to index (that we usually call a vocab) as during pretraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "131ef06c-c91b-43fe-8701-a592c7e874e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/gt/m3tbcpvj4k33rzs972f8vb600000gn/T/ipykernel_62592/681194254.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#  automatically download the vocab used during pretraining or fine-tuning a given model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bert-base-cased\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/pratt_ml/lib/python3.7/site-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0;31m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0mtokenizer_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tokenizer_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m         \u001b[0mconfig_tokenizer_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tokenizer_class\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pratt_ml/lib/python3.7/site-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, **kwargs)\u001b[0m\n\u001b[1;32m    328\u001b[0m             \u001b[0mresume_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_download\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m             \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m         )\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pratt_ml/lib/python3.7/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1408\u001b[0m             \u001b[0muser_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m             \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1410\u001b[0;31m             \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1411\u001b[0m         )\n\u001b[1;32m   1412\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pratt_ml/lib/python3.7/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1664\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{url} not found in cache or force_download set to True, downloading to {temp_file.name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1666\u001b[0;31m             \u001b[0mhttp_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_to_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"storing {url} in cache at {cache_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pratt_ml/lib/python3.7/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers)\u001b[0m\n\u001b[1;32m   1521\u001b[0m         \u001b[0minitial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Downloading\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1523\u001b[0;31m         \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_verbosity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOTSET\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1524\u001b[0m     )\n\u001b[1;32m   1525\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pratt_ml/lib/python3.7/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0munit_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munit_scale\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munit_scale\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0munit_scale\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_printer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mncols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproxy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplayed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pratt_ml/lib/python3.7/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36mstatus_printer\u001b[0;34m(_, total, desc, ncols)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mIProgress\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# #187 #451 #558 #872\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             raise ImportError(\n\u001b[0;32m--> 116\u001b[0;31m                 \u001b[0;34m\"IProgress not found. Please update jupyter and ipywidgets.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m                 \u001b[0;34m\" See https://ipywidgets.readthedocs.io/en/stable\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \"/user_install.html\")\n",
      "\u001b[0;31mImportError\u001b[0m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "#  automatically download the vocab used during pretraining or fine-tuning a given model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfe5951-4123-4988-9aa5-8879e2dba8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "encoded_input = tokenizer(\"I love machine learning!\")\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e62512-673a-4297-a9ab-50333fbffe00",
   "metadata": {},
   "source": [
    "This returns a dictionary string to list of ints. The input_ids are the indices corresponding to each token in our sentence. We will see below what the attention_mask is used for and in the next section the goal of token_type_ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfe6509-0f6f-4480-bc42-5a56aaeed1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(encoded_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b86004-9955-4bc3-a48f-ca50c4bb8bc5",
   "metadata": {},
   "source": [
    "As you can see, the tokenizer automatically added some special tokens that the model expects. Now let's tokenize our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0157ba95-cc61-4141-8c76-1f8c82c8ad29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# This will make all the samples have the maximum length the model can accept (here 512),\n",
    "# either by padding or truncating them. Note that we are applying the preprocessing step to\n",
    "# all splots of the raw dataset\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf61eb8-4759-467e-a2d2-61c74a5e6c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n",
    "# full_train_dataset = tokenized_datasets[\"train\"]\n",
    "# full_eval_dataset = tokenized_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3991a5-c4bb-49b8-9d79-a1f12cf4ded5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", \n",
    "#                                                            num_labels=2)\n",
    "import tensorflow as tf\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", \n",
    "                                                             num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5de5f2-ac87-4751-9c15-fb59e203c49c",
   "metadata": {},
   "source": [
    "Since we are going to train our model natively in TensorFlow, we need to convert our datasets to standard `td.data.Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4464ce22-f03a-432d-9a2e-4d490a795bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_dataset = small_train_dataset.remove_columns([\"text\"]).with_format(\"tensorflow\")\n",
    "tf_eval_dataset = small_eval_dataset.remove_columns([\"text\"]).with_format(\"tensorflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b507f1-2338-4a6d-b055-1f1fd14df8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = {x: tf_train_dataset[x] for x in tokenizer.model_input_names}\n",
    "train_tf_dataset = tf.data.Dataset.from_tensor_slices((train_features, tf_train_dataset[\"label\"]))\n",
    "train_tf_dataset = train_tf_dataset.shuffle(len(tf_train_dataset)).batch(8)\n",
    "\n",
    "eval_features = {x: tf_eval_dataset[x] for x in tokenizer.model_input_names}\n",
    "eval_tf_dataset = tf.data.Dataset.from_tensor_slices((eval_features, tf_eval_dataset[\"label\"]))\n",
    "eval_tf_dataset = eval_tf_dataset.batch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbd4955-8ca3-4718-81dc-7803ce14c177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feba0e18-4618-4315-962a-45909efed3a0",
   "metadata": {},
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=tf.metrics.SparseCategoricalAccuracy(),\n",
    ")\n",
    "\n",
    "model.fit(train_tf_dataset, validation_data=eval_tf_dataset, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8f64b7-d002-4d28-bdee-cea2b4786427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the fine-tuned model for future use\n",
    "# model.save_pretrained(\"./my_imdb_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571764a6-e4b6-40af-9baf-2c5f153a690c",
   "metadata": {},
   "source": [
    "There are many more examples for different tasks sucj as text classification, question answering, etc [here](https://github.com/huggingface/transformers/tree/master/examples/tensorflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2454f68e-93f3-4398-b99b-de265fdeec7d",
   "metadata": {},
   "source": [
    "## optional - HugginFace approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62aa843e-da8c-4fee-93b5-3398fc340b0c",
   "metadata": {},
   "source": [
    "compute_metrics function takes predictions and labels and computes and returns a dictionary with string items (the metric names) and float values (the metric values).\n",
    "\n",
    "The ðŸ¤— Datasets library provides an easy way to get the common metrics used in NLP with the load_metric function. here we simply use accuracy. Then we define the compute_metrics function that just convert logits to predictions (remember that all ðŸ¤— Transformers models return the logits) and feed them to compute method of this metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb87e8b2-0456-46d3-8624-5479010a61d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16f9eb1-7574-46ad-82f1-34262b74d9b2",
   "metadata": {},
   "source": [
    "To define our Trainer, we will need to instantiate a TrainingArguments. This class contains all the hyperparameters we can tune for the Trainer or the flags to activate the different training options it supports. Letâ€™s begin by using all the default arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c92732-905e-41e4-85f0-bb6c04f48cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\"test_trainer\", evaluation_strategy=\"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b32f2fc-1ba6-403c-bb00-d57bc8c426e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "# instantiate a Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args, \n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166109a1-9900-4076-a189-a5fa9d066592",
   "metadata": {},
   "source": [
    "To fine-tune our model, we just need to call\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9cf52c-4563-43e4-bd2c-b563804992fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
